import os
import json
import torch
import numpy as np
from pathlib import Path
from tqdm import tqdm
from collections import defaultdict
from transformers import CLIPTokenizer, CLIPTextModel
from sklearn.metrics.pairwise import cosine_similarity
import warnings

# --- å…¨å±€é…ç½® ---
warnings.filterwarnings("ignore")
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DATA_DIR = "/root/data/cluster_4"
OUTPUT_DIR = "/root/data/cluster_4_restructured_v2"
ANALYSIS_FILE = "tag_analysis_results.json"
TE1_PATH = "/root/text_encoder/clip-vit-large-patch14"
TE2_PATH = "/root/text_encoder/CLIP-ViT-B-32-laion2B-s34B-b79K"

# --- æ ¸å¿ƒæ ‡ç­¾æœ¬ä½“è®º (Ontology) ---
TAG_CATEGORIES_ORDER = [
    "meta", "quality", "style", "space", "lighting", "colors", 
    "materials", "furniture", "atmosphere", "architectural", 
    "decorative", "composition", "environment", "general"
]

TAG_ONTOLOGY = {
    "meta": ["photograph", "image", "shot", "architectural photography"],
    "quality": ["professional", "high quality", "detailed", "sharp", "clear", "elegant", "sophisticated", "stylish", "luxurious"],
    "style": ["modern interior", "contemporary style", "contemporary design", "minimalist", "minimalist design", "minimalistic", "minimalist decor", "contemporary", "modern", "modern architecture", "modern design", "industrial style", "mid-century modern", "sleek design"],
    "space": ["indoor", "living room", "bedroom", "kitchen", "office", "bathroom", "interior design", "empty room", "apartment", "interior", "home decor"],
    "lighting": ["natural light", "soft lighting", "recessed lighting", "sunlight", "warm lighting", "daytime", "ceiling lights", "ambient lighting", "task lighting", "accent lighting"],
    "colors": ["neutral colors", "white walls", "beige walls", "white curtains", "warm tones", "warm colors", "cool tones", "monochromatic", "earth tones", "muted colors"],
    "materials": ["wooden floor", "wooden table", "wooden shelves", "glass doors", "wooden coffee table", "marble floor", "glass coffee table", "glass walls", "concrete floor", "wooden ceiling", "wooden chair", "marble countertop", "textured fabric"],
    "furniture": ["furniture", "sleek furniture", "contemporary furniture", "modern furniture", "comfortable", "round table", "bookshelves", "shelves", "armchair", "desk", "sofa", "chair"],
    "atmosphere": ["cozy atmosphere", "cozy", "clean lines", "sleek", "clean", "spacious", "serene atmosphere", "calm atmosphere", "peaceful", "relaxing", "inviting"],
    "architectural": ["large windows", "large window", "high ceiling", "glass door", "glass window", "open floor plan", "vaulted ceiling", "exposed beams", "floor-to-ceiling windows"],
    "decorative": ["books", "potted plant", "abstract art", "indoor plants", "artwork", "decorative objects", "vase", "flowers", "plants", "greenery"],
    "composition": ["shadows", "geometric shapes", "symmetry", "asymmetrical", "vertical composition", "horizontal composition", "perspective", "depth of field", "framing"],
    "environment": ["urban setting", "cityscape view", "urban view", "city view", "outdoor view", "ocean view", "nature view", "garden view", "balcony", "terrace"],
    "general": []
}

class TagRestructuringV2:
    """
    ä¸€ä¸ªé›†æˆäº†è¯­ä¹‰é‡æ’åºã€ç²¾ç¡®Tokenè®¡ç®—å’Œè¯­ä¹‰ä¿çœŸåº¦éªŒè¯çš„
    é«˜çº§æ•°æ®é‡æ„å·¥å…·ã€‚
    """
    def __init__(self):
        print("[INFO] æ­£åœ¨åˆå§‹åŒ–æ•°æ®é‡æ„å·¥å…· V2.0...")
        self._load_models()
        self._build_tag_mapping()

    def _load_models(self):
        """åŠ è½½TE1å’ŒTE2çš„æ¨¡å‹ä¸åˆ†è¯å™¨"""
        print(f"[INFO] ä½¿ç”¨è®¾å¤‡: {DEVICE}")
        self.te1_tokenizer = CLIPTokenizer.from_pretrained(TE1_PATH)
        self.te1_model = CLIPTextModel.from_pretrained(TE1_PATH).to(DEVICE).eval()
        self.te2_tokenizer = CLIPTokenizer.from_pretrained(TE2_PATH)
        self.te2_model = CLIPTextModel.from_pretrained(TE2_PATH).to(DEVICE).eval()
        print("[INFO] TE1 å’Œ TE2 æ¨¡å‹åŠ è½½å®Œæ¯•ã€‚")

    def _build_tag_mapping(self):
        """æ„å»ºæ ‡ç­¾åˆ°ç±»åˆ«çš„æ˜ å°„"""
        self.tag_to_category = {}
        for category, tags in TAG_ONTOLOGY.items():
            for tag in tags:
                self.tag_to_category[tag] = category

    @torch.no_grad()
    def _get_embedding(self, text, model_type="te1"):
        """è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        if model_type == "te1":
            tokenizer = self.te1_tokenizer
            model = self.te1_model
        else:
            tokenizer = self.te2_tokenizer
            model = self.te2_model
        
        inputs = tokenizer(text, padding=True, truncation=True, return_tensors="pt", max_length=77).to(DEVICE)
        return model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()

    def _categorize_and_reorder_tags(self, tags):
        """å¯¹æ ‡ç­¾è¿›è¡Œåˆ†ç±»å’Œé‡æ’åºï¼Œä¸æ·»åŠ é¢å¤–æ ‡è®°"""
        categorized = defaultdict(list)
        for tag in tags:
            tag_lower = tag.lower().strip()
            found_category = self.tag_to_category.get(tag_lower, "general")
            categorized[found_category].append(tag)
        
        reordered_tags = []
        for category in TAG_CATEGORIES_ORDER:
            if category in categorized:
                # ä¿æŒåŸå§‹å¤§å°å†™ï¼Œå»é‡
                unique_tags = sorted(list(set(categorized[category])), key=lambda x: tags.index(x))
                reordered_tags.extend(unique_tags)
        
        return ", ".join(reordered_tags)
    
    def get_token_counts(self, text):
        """ä½¿ç”¨çœŸå®åˆ†è¯å™¨è®¡ç®—Tokenæ•°é‡"""
        tokens1 = self.te1_tokenizer(text, truncation=False, add_special_tokens=True)['input_ids']
        tokens2 = self.te2_tokenizer(text, truncation=False, add_special_tokens=True)['input_ids']
        return len(tokens1), len(tokens2)

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„æ•°æ®é‡æ„å’ŒéªŒè¯æµç¨‹"""
        if not os.path.exists(OUTPUT_DIR):
            os.makedirs(OUTPUT_DIR)
            print(f"[INFO] å·²åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

        all_txt_files = list(Path(DATA_DIR).glob("*.txt"))
        print(f"[INFO] å‘ç° {len(all_txt_files)} ä¸ªæ ‡ç­¾æ–‡ä»¶éœ€è¦å¤„ç†ã€‚")

        report_data = []
        pbar = tqdm(all_txt_files, desc="[V2.0] æ•°æ®é‡æ„ä¸éªŒè¯")
        for txt_file in pbar:
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                
                if not lines or not lines[0].strip():
                    continue

                original_tags_str = lines[0].strip()
                description = lines[1].strip() if len(lines) > 1 else ""
                original_tags_list = [t.strip() for t in original_tags_str.split(',') if t.strip()]

                # 1. é‡æ’åº
                reordered_tags_str = self._categorize_and_reorder_tags(original_tags_list)
                
                # 2. è®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦
                original_emb1 = self._get_embedding(original_tags_str, "te1")
                reordered_emb1 = self._get_embedding(reordered_tags_str, "te1")
                sim1 = cosine_similarity(original_emb1, reordered_emb1)[0][0]
                
                original_emb2 = self._get_embedding(original_tags_str, "te2")
                reordered_emb2 = self._get_embedding(reordered_tags_str, "te2")
                sim2 = cosine_similarity(original_emb2, reordered_emb2)[0][0]

                # 3. è®¡ç®—ç²¾ç¡®Tokené•¿åº¦
                tokens1, tokens2 = self.get_token_counts(reordered_tags_str)

                # 4. ä¿å­˜é‡æ„æ–‡ä»¶
                with open(Path(OUTPUT_DIR) / txt_file.name, 'w', encoding='utf-8') as f:
                    f.write(reordered_tags_str + '\n')
                    if description:
                        f.write(description + '\n')

                report_data.append({
                    "filename": txt_file.name,
                    "similarity_te1": float(sim1),
                    "similarity_te2": float(sim2),
                    "tokens_te1": tokens1,
                    "tokens_te2": tokens2,
                    "original_length": len(original_tags_str),
                    "reordered_length": len(reordered_tags_str)
                })

            except Exception as e:
                print(f"[ERROR] å¤„ç†æ–‡ä»¶ {txt_file.name} å¤±è´¥: {e}")

        # 5. ç”ŸæˆæŠ¥å‘Š
        self._generate_report(report_data)

    def _generate_report(self, report_data):
        """ç”Ÿæˆæœ€ç»ˆçš„é‡åŒ–åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“Š SDXLæ•°æ®é‡æ„V2.0 - æœ€ç»ˆé‡åŒ–æŠ¥å‘Š")
        print("="*80)

        if not report_data:
            print("[ERROR] æ²¡æœ‰å¤„ç†ä»»ä½•æ–‡ä»¶ï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Šã€‚")
            return

        avg_sim1 = np.mean([d['similarity_te1'] for d in report_data])
        avg_sim2 = np.mean([d['similarity_te2'] for d in report_data])
        avg_tokens1 = np.mean([d['tokens_te1'] for d in report_data])
        avg_tokens2 = np.mean([d['tokens_te2'] for d in report_data])
        max_tokens1 = max([d['tokens_te1'] for d in report_data])
        max_tokens2 = max([d['tokens_te2'] for d in report_data])

        print(f"      å¤„ç†æ–‡ä»¶æ€»æ•°: {len(report_data)}")
        print("\n--- è¯­ä¹‰ä¿çœŸåº¦ (è¶Šé«˜è¶Šå¥½) ---")
        print(f"      TE1 å¹³å‡ç›¸ä¼¼åº¦: {avg_sim1:.4f}")
        print(f"      TE2 å¹³å‡ç›¸ä¼¼åº¦: {avg_sim2:.4f}")
        print(f"      ç»¼åˆå¹³å‡ç›¸ä¼¼åº¦: {(avg_sim1 + avg_sim2) / 2:.4f}")
        
        print("\n--- Tokené•¿åº¦åˆ†æ (è¶Šä½è¶Šå¥½) ---")
        print(f"      TE1 å¹³å‡Tokenæ•°: {avg_tokens1:.1f} (æœ€å¤§: {max_tokens1})")
        print(f"      TE2 å¹³å‡Tokenæ•°: {avg_tokens2:.1f} (æœ€å¤§: {max_tokens2})")

        # å°†æŠ¥å‘Šä¿å­˜åˆ°JSONæ–‡ä»¶
        report_path = "data_reconstruction_report_v2.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                "summary": {
                    "file_count": len(report_data),
                    "avg_similarity_te1": avg_sim1,
                    "avg_similarity_te2": avg_sim2,
                    "avg_tokens_te1": avg_tokens1,
                    "max_tokens_te1": max_tokens1,
                    "avg_tokens_te2": avg_tokens2,
                    "max_tokens_te2": max_tokens2
                },
                "details": report_data
            }, f, indent=2, ensure_ascii=False)
        
        print(f"\n[SUCCESS] V2.0æ•°æ®é‡æ„å®Œæˆï¼è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_path}")
        print("="*80)


if __name__ == "__main__":
    restructurer = TagRestructuringV2()
    restructurer.run() 