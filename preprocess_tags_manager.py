import os
import json
import argparse
import subprocess
import time
import math
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm
import multiprocessing
import sys

class TagProcessingManager:
    """
    æ ‡ç­¾å¤„ç†ç®¡ç†å™¨ - æ™ºèƒ½æ‰¹æ¬¡åˆ†ç‰‡å’Œå¤šè¿›ç¨‹ç®¡ç†
    """
    
    def __init__(self, source_dir, output_dir, max_workers=None, batch_size=None):
        self.source_dir = Path(source_dir)
        self.output_dir = Path(output_dir)
        
        # æ™ºèƒ½è®¾ç½®æœ€å¤§å·¥ä½œè¿›ç¨‹æ•°
        if max_workers is None:
            # åŸºäºGPUæ•°é‡å’ŒCPUæ ¸å¿ƒæ•°æ™ºèƒ½è®¾ç½®
            cpu_count = multiprocessing.cpu_count()
            # å¯¹äºGPUå¯†é›†å‹ä»»åŠ¡ï¼Œé€šå¸¸4-6ä¸ªè¿›ç¨‹æ¯”è¾ƒåˆé€‚
            self.max_workers = min(6, max(2, cpu_count // 2))
        else:
            self.max_workers = max_workers
            
        # è‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°
        self.batch_size = self._calculate_optimal_batch_size() if batch_size is None else batch_size
        
        print(f"[MANAGER] åˆå§‹åŒ–å¤„ç†ç®¡ç†å™¨")
        print(f"[MANAGER] æºç›®å½•: {self.source_dir}")
        print(f"[MANAGER] è¾“å‡ºç›®å½•: {self.output_dir}")
        print(f"[MANAGER] æœ€å¤§å¹¶å‘è¿›ç¨‹: {self.max_workers}")
        print(f"[MANAGER] æ™ºèƒ½æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _calculate_optimal_batch_size(self):
        """åŸºäºæ•°æ®é‡å’Œç¡¬ä»¶è‡ªåŠ¨è®¡ç®—æœ€ä¼˜æ‰¹æ¬¡å¤§å°"""
        # å‘ç°æ‰€æœ‰txtæ–‡ä»¶
        txt_files = list(self.source_dir.glob("*.txt"))
        total_files = len(txt_files)
        
        if total_files == 0:
            print("[MANAGER ERROR] æœªæ‰¾åˆ°ä»»ä½•txtæ–‡ä»¶")
            return 50
        
        # æ™ºèƒ½æ‰¹æ¬¡å¤§å°è®¡ç®—é€»è¾‘
        if total_files <= 100:
            # å°æ•°æ®é›†ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†10-20ä¸ªæ–‡ä»¶
            batch_size = max(10, total_files // self.max_workers)
        elif total_files <= 500:
            # ä¸­ç­‰æ•°æ®é›†ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†20-50ä¸ªæ–‡ä»¶
            batch_size = max(20, total_files // (self.max_workers * 2))
        elif total_files <= 2000:
            # å¤§æ•°æ®é›†ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†50-100ä¸ªæ–‡ä»¶
            batch_size = max(50, total_files // (self.max_workers * 3))
        else:
            # è¶…å¤§æ•°æ®é›†ï¼šæ¯ä¸ªè¿›ç¨‹å¤„ç†100-200ä¸ªæ–‡ä»¶
            batch_size = max(100, min(200, total_files // (self.max_workers * 4)))
        
        # ç¡®ä¿æ‰¹æ¬¡å¤§å°åˆç†
        batch_size = min(batch_size, total_files)
        
        print(f"[MANAGER] æ£€æµ‹åˆ° {total_files} ä¸ªæ–‡ä»¶ï¼Œè®¡ç®—å‡ºæœ€ä¼˜æ‰¹æ¬¡å¤§å°: {batch_size}")
        return batch_size

    def _discover_files(self):
        """å‘ç°å¹¶åˆ†ææ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶"""
        txt_files = list(self.source_dir.glob("*.txt"))
        
        if not txt_files:
            print("[MANAGER ERROR] æºç›®å½•ä¸­æœªæ‰¾åˆ°ä»»ä½•txtæ–‡ä»¶")
            return []
        
        print(f"[MANAGER] å‘ç° {len(txt_files)} ä¸ªæ–‡ä»¶å¾…å¤„ç†")
        
        # è¿”å›æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²åˆ—è¡¨
        return [str(f) for f in txt_files]

    def _create_batches(self, file_list):
        """å°†æ–‡ä»¶åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡"""
        batches = []
        
        for i in range(0, len(file_list), self.batch_size):
            batch = file_list[i:i + self.batch_size]
            batches.append(batch)
        
        print(f"[MANAGER] åˆ›å»ºäº† {len(batches)} ä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹æ¬¡çº¦ {self.batch_size} ä¸ªæ–‡ä»¶")
        return batches

    def _run_batch_process(self, batch_files, batch_id):
        """è¿è¡Œå•ä¸ªæ‰¹æ¬¡çš„subprocess"""
        try:
            # æ„å»ºå‘½ä»¤è¡Œå‚æ•°
            batch_files_str = ",".join(batch_files)
            
            cmd = [
                sys.executable,  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
                "preprocess_tags_batch.py",
                "--batch-files", batch_files_str,
                "--source-dir", str(self.source_dir),
                "--output-dir", str(self.output_dir),
                "--batch-id", str(batch_id),
                "--max-resolution", "5000"
            ]
            
            print(f"[MANAGER] å¯åŠ¨æ‰¹æ¬¡ {batch_id} (æ–‡ä»¶æ•°: {len(batch_files)})")
            
            # è¿è¡Œsubprocess
            start_time = time.time()
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=3600  # 1å°æ—¶è¶…æ—¶
            )
            
            end_time = time.time()
            process_time = end_time - start_time
            
            if result.returncode == 0:
                print(f"[MANAGER] æ‰¹æ¬¡ {batch_id} å®Œæˆ (è€—æ—¶: {process_time:.1f}s)")
                return {
                    "batch_id": batch_id,
                    "status": "success",
                    "process_time": process_time,
                    "files_count": len(batch_files),
                    "stdout": result.stdout[-500:] if result.stdout else "",  # åªä¿ç•™æœ€å500å­—ç¬¦
                    "stderr": result.stderr[-500:] if result.stderr else ""
                }
            else:
                print(f"[MANAGER ERROR] æ‰¹æ¬¡ {batch_id} å¤±è´¥ (è¿”å›ç : {result.returncode})")
                print(f"[MANAGER ERROR] é”™è¯¯è¾“å‡º: {result.stderr}")
                return {
                    "batch_id": batch_id,
                    "status": "failed",
                    "process_time": process_time,
                    "files_count": len(batch_files),
                    "returncode": result.returncode,
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
                
        except subprocess.TimeoutExpired:
            print(f"[MANAGER ERROR] æ‰¹æ¬¡ {batch_id} è¶…æ—¶")
            return {
                "batch_id": batch_id,
                "status": "timeout",
                "files_count": len(batch_files),
                "error": "Process timeout"
            }
        except Exception as e:
            print(f"[MANAGER ERROR] æ‰¹æ¬¡ {batch_id} å¼‚å¸¸: {e}")
            return {
                "batch_id": batch_id,
                "status": "error",
                "files_count": len(batch_files),
                "error": str(e)
            }

    def _collect_batch_results(self):
        """æ”¶é›†æ‰€æœ‰æ‰¹æ¬¡çš„ç»“æœ"""
        batch_result_files = list(self.output_dir.glob("batch_*_results.json"))
        
        if not batch_result_files:
            print("[MANAGER ERROR] æœªæ‰¾åˆ°ä»»ä½•æ‰¹æ¬¡ç»“æœæ–‡ä»¶")
            return None
        
        print(f"[MANAGER] æ”¶é›† {len(batch_result_files)} ä¸ªæ‰¹æ¬¡ç»“æœ")
        
        all_results = []
        total_stats = {
            "found": 0, "missing": 0, "processed": 0, 
            "errors": 0, "compressed": 0, "total_size_reduction": 0
        }
        
        for result_file in batch_result_files:
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                
                # ç´¯è®¡ç»Ÿè®¡æ•°æ®
                image_stats = batch_data.get("image_stats", {})
                for key in total_stats:
                    total_stats[key] += image_stats.get(key, 0)
                
                # æ”¶é›†è¯¦ç»†ç»“æœ
                all_results.extend(batch_data.get("results", []))
                
            except Exception as e:
                print(f"[MANAGER ERROR] è¯»å–æ‰¹æ¬¡ç»“æœæ–‡ä»¶ {result_file} å¤±è´¥: {e}")
        
        return all_results, total_stats

    def _generate_final_report(self, all_results, total_stats, process_info):
        """ç”Ÿæˆæœ€ç»ˆçš„ç»¼åˆæŠ¥å‘Š"""
        if not all_results:
            print("[MANAGER ERROR] æ²¡æœ‰æœ‰æ•ˆçš„å¤„ç†ç»“æœ")
            return
        
        # è®¡ç®—å¹³å‡å€¼
        import numpy as np
        
        avg_sim1 = np.mean([d['similarity_te1'] for d in all_results if 'similarity_te1' in d])
        avg_sim2 = np.mean([d['similarity_te2'] for d in all_results if 'similarity_te2' in d])
        avg_tokens1 = np.mean([d['tokens_te1'] for d in all_results if 'tokens_te1' in d])
        avg_tokens2 = np.mean([d['tokens_te2'] for d in all_results if 'tokens_te2' in d])
        max_tokens1 = max([d['tokens_te1'] for d in all_results if 'tokens_te1' in d], default=0)
        max_tokens2 = max([d['tokens_te2'] for d in all_results if 'tokens_te2' in d], default=0)
        
        # è®¡ç®—æ€»ä½“å¤„ç†æ€§èƒ½
        total_process_time = sum([p.get("process_time", 0) for p in process_info])
        successful_batches = len([p for p in process_info if p.get("status") == "success"])
        failed_batches = len([p for p in process_info if p.get("status") != "success"])
        
        print("\n" + "="*80)
        print("ğŸš€ SDXLæ•°æ®é‡æ„ - å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†æŠ¥å‘Š")
        print("="*80)
        
        print(f"ğŸ“Š å¤„ç†æ€§èƒ½ç»Ÿè®¡:")
        print(f"      æ€»æ–‡ä»¶æ•°: {len(all_results)}")
        print(f"      å¤„ç†æ‰¹æ¬¡æ•°: {len(process_info)}")
        print(f"      æˆåŠŸæ‰¹æ¬¡: {successful_batches}")
        print(f"      å¤±è´¥æ‰¹æ¬¡: {failed_batches}")
        print(f"      å¹¶å‘è¿›ç¨‹æ•°: {self.max_workers}")
        print(f"      æ‰¹æ¬¡å¤§å°: {self.batch_size}")
        print(f"      æ€»å¤„ç†æ—¶é—´: {total_process_time:.1f} ç§’")
        if len(all_results) > 0:
            avg_speed = len(all_results) / total_process_time if total_process_time > 0 else 0
            print(f"      å¤„ç†é€Ÿåº¦: {avg_speed:.2f} æ–‡ä»¶/ç§’")
        
        print(f"\nğŸ–¼ï¸ å›¾ç‰‡å¤„ç†ç»Ÿè®¡:")
        print(f"      æ‰¾åˆ°å›¾ç‰‡: {total_stats['found']}")
        print(f"      ç¼ºå¤±å›¾ç‰‡: {total_stats['missing']}")
        print(f"      æˆåŠŸå¤„ç†: {total_stats['processed']}")
        print(f"      å¤„ç†é”™è¯¯: {total_stats['errors']}")
        pairing_rate = total_stats['found']/(total_stats['found']+total_stats['missing'])*100 if (total_stats['found']+total_stats['missing']) > 0 else 0
        print(f"      é…å¯¹æˆåŠŸç‡: {pairing_rate:.1f}%")
        
        print(f"\nğŸ—œï¸ å›¾ç‰‡å‹ç¼©ç»Ÿè®¡:")
        print(f"      å‹ç¼©å›¾ç‰‡æ•°é‡: {total_stats['compressed']}")
        compression_rate = total_stats['compressed']/max(total_stats['found'], 1)*100
        print(f"      å‹ç¼©ç‡: {compression_rate:.1f}%")
        if total_stats['compressed'] > 0:
            avg_size_reduction = total_stats['total_size_reduction'] / total_stats['compressed'] * 100
            print(f"      å¹³å‡åƒç´ å‡å°‘: {avg_size_reduction:.1f}%")
        
        print(f"\nğŸ”¤ è¯­ä¹‰ä¿çœŸåº¦åˆ†æ:")
        print(f"      TE1 å¹³å‡ç›¸ä¼¼åº¦: {avg_sim1:.4f}")
        print(f"      TE2 å¹³å‡ç›¸ä¼¼åº¦: {avg_sim2:.4f}")
        print(f"      ç»¼åˆå¹³å‡ç›¸ä¼¼åº¦: {(avg_sim1 + avg_sim2) / 2:.4f}")
        
        print(f"\nğŸ“ Tokené•¿åº¦åˆ†æ:")
        print(f"      TE1 å¹³å‡Tokenæ•°: {avg_tokens1:.1f} (æœ€å¤§: {max_tokens1})")
        print(f"      TE2 å¹³å‡Tokenæ•°: {avg_tokens2:.1f} (æœ€å¤§: {max_tokens2})")
        
        # ä¿å­˜æœ€ç»ˆæŠ¥å‘Š
        final_report = {
            "summary": {
                "processing_mode": "å¤šè¿›ç¨‹å¹¶è¡Œå¤„ç†",
                "total_files": len(all_results),
                "batch_count": len(process_info),
                "successful_batches": successful_batches,
                "failed_batches": failed_batches,
                "max_workers": self.max_workers,
                "batch_size": self.batch_size,
                "total_process_time": total_process_time,
                "processing_speed": len(all_results) / total_process_time if total_process_time > 0 else 0,
                "image_stats": total_stats,
                "avg_similarity_te1": float(avg_sim1),
                "avg_similarity_te2": float(avg_sim2),
                "avg_tokens_te1": float(avg_tokens1),
                "max_tokens_te1": int(max_tokens1),
                "avg_tokens_te2": float(avg_tokens2),
                "max_tokens_te2": int(max_tokens2)
            },
            "batch_info": process_info,
            "detailed_results": all_results
        }
        
        final_report_path = self.output_dir / "final_processing_report.json"
        with open(final_report_path, 'w', encoding='utf-8') as f:
            json.dump(final_report, f, indent=2, ensure_ascii=False)
        
        print(f"\n[SUCCESS] å¤šè¿›ç¨‹å¤„ç†å®Œæˆï¼æœ€ç»ˆæŠ¥å‘Šå·²ä¿å­˜è‡³: {final_report_path}")
        print("="*80)

    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„å¤šè¿›ç¨‹å¤„ç†æµç¨‹"""
        print(f"[MANAGER] å¼€å§‹å¤šè¿›ç¨‹æ ‡ç­¾å¤„ç†...")
        
        # 1. å‘ç°æ–‡ä»¶
        file_list = self._discover_files()
        if not file_list:
            return
        
        # 2. åˆ›å»ºæ‰¹æ¬¡
        batches = self._create_batches(file_list)
        
        # 3. å¹¶è¡Œå¤„ç†æ‰¹æ¬¡
        process_info = []
        
        print(f"[MANAGER] å¼€å§‹å¹¶è¡Œå¤„ç† {len(batches)} ä¸ªæ‰¹æ¬¡...")
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡ä»»åŠ¡
            future_to_batch = {
                executor.submit(self._run_batch_process, batch, i): i 
                for i, batch in enumerate(batches)
            }
            
            # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»ä½“è¿›åº¦
            with tqdm(total=len(batches), desc="æ‰¹æ¬¡å¤„ç†è¿›åº¦", unit="æ‰¹æ¬¡") as pbar:
                for future in as_completed(future_to_batch):
                    batch_id = future_to_batch[future]
                    try:
                        result = future.result()
                        process_info.append(result)
                        
                        # æ›´æ–°è¿›åº¦æ¡æè¿°
                        if result["status"] == "success":
                            pbar.set_postfix({
                                "æˆåŠŸ": len([p for p in process_info if p.get("status") == "success"]),
                                "å¤±è´¥": len([p for p in process_info if p.get("status") != "success"])
                            })
                        
                    except Exception as e:
                        print(f"[MANAGER ERROR] æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå¼‚å¸¸: {e}")
                        process_info.append({
                            "batch_id": batch_id,
                            "status": "exception",
                            "error": str(e)
                        })
                    
                    pbar.update(1)
        
        # 4. æ”¶é›†ç»“æœ
        print(f"[MANAGER] æ”¶é›†å¤„ç†ç»“æœ...")
        results_data = self._collect_batch_results()
        if results_data is None:
            print("[MANAGER ERROR] æœªèƒ½æ”¶é›†åˆ°æœ‰æ•ˆç»“æœ")
            return
        
        all_results, total_stats = results_data
        
        # 5. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        self._generate_final_report(all_results, total_stats, process_info)
        
        # 6. æ¸…ç†ä¸´æ—¶æ‰¹æ¬¡ç»“æœæ–‡ä»¶
        print(f"[MANAGER] æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
        for result_file in self.output_dir.glob("batch_*_results.json"):
            try:
                result_file.unlink()
            except Exception as e:
                print(f"[MANAGER WARNING] æ¸…ç†æ–‡ä»¶ {result_file} å¤±è´¥: {e}")


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="å¤šè¿›ç¨‹æ ‡ç­¾å¤„ç†ç®¡ç†å™¨")
    parser.add_argument("--source-dir", required=True,
                       help="æºæ•°æ®ç›®å½•")
    parser.add_argument("--output-dir", required=True,
                       help="è¾“å‡ºç›®å½•")
    parser.add_argument("--max-workers", type=int, default=None,
                       help="æœ€å¤§å¹¶å‘è¿›ç¨‹æ•° (é»˜è®¤: è‡ªåŠ¨æ£€æµ‹)")
    parser.add_argument("--batch-size", type=int, default=None,
                       help="æ‰¹æ¬¡å¤§å° (é»˜è®¤: è‡ªåŠ¨è®¡ç®—)")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()
    
    print(f"[MANAGER] å¯åŠ¨å¤šè¿›ç¨‹æ ‡ç­¾å¤„ç†ç®¡ç†å™¨")
    print(f"[MANAGER] æºç›®å½•: {args.source_dir}")
    print(f"[MANAGER] è¾“å‡ºç›®å½•: {args.output_dir}")
    
    # åˆ›å»ºç®¡ç†å™¨å¹¶æ‰§è¡Œ
    manager = TagProcessingManager(
        source_dir=args.source_dir,
        output_dir=args.output_dir,
        max_workers=args.max_workers,
        batch_size=args.batch_size
    )
    
    start_time = time.time()
    manager.run()
    end_time = time.time()
    
    print(f"\n[MANAGER] æ€»æ‰§è¡Œæ—¶é—´: {end_time - start_time:.1f} ç§’")
    print(f"[MANAGER] å¤šè¿›ç¨‹å¤„ç†å®Œæˆï¼") 