#!/usr/bin/env python3
"""
SDXLè®­ç»ƒç–‘æƒ‘ç»¼åˆè§£è°œè„šæœ¬
åŸºäºå®éªŒéªŒè¯æ–‡æ¡£ä¸­æå‡ºä½†æœªå®Œå…¨è§£å†³çš„å…³é”®é—®é¢˜
"""

import torch
import numpy as np
from transformers import CLIPTokenizer, CLIPTextModel
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
import json
import random
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import time
from tqdm import tqdm
import gc

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class MysteryAnalyzer:
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.load_models()
        
    def load_models(self):
        """åŠ è½½TE1å’ŒTE2æ¨¡å‹"""
        model_configs = {
            "TE1": "/root/text_encoder/CLIP-ViT-B-32-laion2B-s34B-b79K",
            "TE2": "/root/text_encoder/clip-vit-large-patch14"
        }
        
        for name, path in model_configs.items():
            print(f"åŠ è½½ {name} æ¨¡å‹...")
            self.tokenizers[name] = CLIPTokenizer.from_pretrained(path)
            self.models[name] = CLIPTextModel.from_pretrained(path).to(device)
            self.models[name].eval()
        
        print(f"æ‰€æœ‰æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    def encode_text_with_details(self, text, model_name="TE1", max_length=77):
        """è¯¦ç»†ç¼–ç æ–‡æœ¬ï¼Œè¿”å›åµŒå…¥å’Œtokenä¿¡æ¯"""
        tokenizer = self.tokenizers[model_name]
        model = self.models[model_name]
        
        # åˆ†è¯
        tokens = tokenizer(text, truncation=True, padding="max_length", 
                          max_length=max_length, return_tensors="pt")
        
        # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡
        input_ids = tokens['input_ids'][0]
        valid_tokens = (input_ids != tokenizer.pad_token_id).sum().item()
        
        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        original_tokens = tokenizer(text, truncation=False)['input_ids']
        is_truncated = len(original_tokens) > max_length
        
        # ç¼–ç 
        with torch.no_grad():
            tokens = tokens.to(device)
            outputs = model(**tokens)
            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
        
        return {
            'embedding': embedding,
            'valid_tokens': valid_tokens,
            'is_truncated': is_truncated,
            'original_length': len(original_tokens),
            'tokens_preview': tokenizer.decode(input_ids[:10]) if valid_tokens > 0 else ""
        }
    
    def encode_batch_texts(self, texts, model_name="TE1", max_length=77, batch_size=32):
        """æ‰¹é‡ç¼–ç æ–‡æœ¬ï¼Œæé«˜å¤„ç†æ•ˆç‡"""
        tokenizer = self.tokenizers[model_name]
        model = self.models[model_name]
        
        all_embeddings = []
        all_details = []
        
        for i in tqdm(range(0, len(texts), batch_size), desc=f"æ‰¹é‡ç¼–ç  {model_name}"):
            batch_texts = texts[i:i+batch_size]
            
            # æ‰¹é‡åˆ†è¯
            batch_tokens = tokenizer(batch_texts, truncation=True, padding="max_length", 
                                   max_length=max_length, return_tensors="pt")
            
            # æ‰¹é‡ç¼–ç 
            with torch.no_grad():
                batch_tokens = batch_tokens.to(device)
                outputs = model(**batch_tokens)
                batch_embeddings = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
                all_embeddings.append(batch_embeddings)
            
            # è®¡ç®—è¯¦ç»†ä¿¡æ¯
            for j, text in enumerate(batch_texts):
                input_ids = batch_tokens['input_ids'][j]
                valid_tokens = (input_ids != tokenizer.pad_token_id).sum().item()
                original_tokens = tokenizer(text, truncation=False)['input_ids']
                is_truncated = len(original_tokens) > max_length
                
                all_details.append({
                    'valid_tokens': valid_tokens,
                    'is_truncated': is_truncated,
                    'original_length': len(original_tokens)
                })
            
            # æ¸…ç†GPUå†…å­˜
            del batch_tokens, outputs
            torch.cuda.empty_cache()
        
        # åˆå¹¶æ‰€æœ‰åµŒå…¥
        final_embeddings = np.vstack(all_embeddings)
        
        return final_embeddings, all_details

    def mystery_1_text_length_impact(self):
        """é‡æ–°è®¾è®¡çš„ç–‘æƒ‘1æµ‹è¯•ï¼šçœŸæ­£å¯¹æ¯”77æˆªæ–­ vs 225åˆ†å—çš„å·®å¼‚"""
        print("\n" + "="*60)
        print("ğŸ” SDXLè®­ç»ƒä¸­çš„Tokenå¤„ç†æœºåˆ¶æ·±åº¦å¯¹æ¯”æµ‹è¯•")
        print("="*60)
        
        # æµ‹è¯•æ ·æœ¬å®šä¹‰
        test_cases = [
            {
                "id": 1,
                "name": "çŸ­æ–‡æœ¬",
                "text": "1girl, solo, beautiful, detailed eyes",
                "expected_behavior": "å•å—å¤„ç†ï¼Œæ— æˆªæ–­"
            },
            {
                "id": 2, 
                "name": "ä¸­ç­‰æ–‡æœ¬",
                "text": "1girl, solo, beautiful detailed eyes, long flowing hair, elegant dress, standing in garden, sunlight filtering through leaves",
                "expected_behavior": "å•å—å¤„ç†ï¼Œå¯èƒ½è½»å¾®æˆªæ–­"
            },
            {
                "id": 3,
                "name": "æ¥è¿‘77é™åˆ¶",
                "text": "1girl, solo, beautiful detailed sparkling blue eyes, long flowing golden hair with intricate braids, elegant white dress with lace details, standing gracefully",
                "expected_behavior": "å•å—å¤„ç†ï¼Œæ¥è¿‘é™åˆ¶"
            },
            {
                "id": 4,
                "name": "è¶…å‡º77é™åˆ¶",
                "text": "1girl, solo, beautiful detailed sparkling blue eyes with long eyelashes, long flowing golden hair with intricate braids and ribbons, elegant white dress with delicate lace details and embroidery, standing gracefully in a beautiful garden with blooming flowers and ancient trees, soft sunlight filtering through the leaves creating dappled shadows",
                "expected_behavior": "éœ€è¦åˆ†å—å¤„ç†"
            },
            {
                "id": 5,
                "name": "å¤§å¹…è¶…å‡º",
                "text": "1girl, solo, beautiful detailed sparkling blue eyes with long dark eyelashes, long flowing golden hair with intricate braids decorated with ribbons and flowers, elegant white dress with delicate lace details and golden embroidery patterns, standing gracefully in a beautiful enchanted garden with blooming roses and ancient oak trees, soft golden sunlight filtering through the green leaves creating magical dappled shadows on the ground, gentle breeze moving her hair and dress, peaceful serene expression with a subtle smile, high quality, masterpiece, ultra detailed, 8k resolution, professional photography lighting",
                "expected_behavior": "å¼ºåˆ¶éœ€è¦3å—åˆ†ç»„å¤„ç†"
            }
        ]
        
        print("ğŸ“‹ æµ‹è¯•æ ·æœ¬æ¦‚è§ˆ:")
        print(f"{'ID':<3} {'ç±»å‹':<12} {'åŸå§‹é•¿åº¦':<8} {'é¢„æœŸè¡Œä¸º':<20} {'æ–‡æœ¬é¢„è§ˆ'}")
        print("-" * 80)
        for case in test_cases:
            tokens = self.tokenize_text_te1(case["text"])
            preview = case["text"][:30] + "..." if len(case["text"]) > 30 else case["text"]
            print(f"{case['id']:<3} {case['name']:<12} {len(tokens):<8} {case['expected_behavior']:<20} {preview}")
        
        print("\n" + "="*60)
        print("ğŸ”„ å¼€å§‹å¯¹æ¯”æµ‹è¯•: 77æˆªæ–­ vs 225åˆ†å—")
        print("="*60)
        
        results = []
        
        for case in test_cases:
            print(f"\nğŸ” æµ‹è¯•æ ·æœ¬ {case['id']}: {case['name']}")
            print("-" * 40)
            
            # æ–¹æ³•1: ä¼ ç»Ÿ77-tokenæˆªæ–­
            te1_77_tokens = self.tokenize_text_te1(case["text"])
            te2_77_tokens = self.tokenize_text_te2(case["text"])
            
            # æˆªæ–­åˆ°77
            te1_77_truncated = te1_77_tokens[:77] if len(te1_77_tokens) > 77 else te1_77_tokens
            te2_77_truncated = te2_77_tokens[:77] if len(te2_77_tokens) > 77 else te2_77_tokens
            
            # ç¼–ç 
            with torch.no_grad():
                te1_77_hidden = self.te1_model(torch.tensor([te1_77_truncated]).to(self.device)).last_hidden_state
                te2_77_hidden = self.te2_model(torch.tensor([te2_77_truncated]).to(self.device)).last_hidden_state
            
            # æ–¹æ³•2: SDXL 225åˆ†å—å¤„ç†
            te1_225_hidden, te2_225_hidden = self.simulate_real_sdxl_chunking(case["text"])
            
            # è®¡ç®—ä¿¡æ¯ä¿ç•™åº¦
            te1_similarity = torch.nn.functional.cosine_similarity(
                te1_77_hidden.mean(dim=1), te1_225_hidden.mean(dim=1), dim=1
            ).item()
            
            te2_similarity = torch.nn.functional.cosine_similarity(
                te2_77_hidden.mean(dim=1), te2_225_hidden.mean(dim=1), dim=1
            ).item()
            
            # è®°å½•ç»“æœ
            result = {
                "id": case["id"],
                "name": case["name"],
                "original_length": len(self.tokenize_text_te1(case["text"])),
                "te1_77_length": len(te1_77_truncated),
                "te2_77_length": len(te2_77_truncated),
                "te1_truncated": len(te1_77_tokens) > 77,
                "te2_truncated": len(te2_77_tokens) > 77,
                "te1_similarity": te1_similarity,
                "te2_similarity": te2_similarity,
                "te1_info_loss": (1 - te1_similarity) * 100,
                "te2_info_loss": (1 - te2_similarity) * 100
            }
            results.append(result)
            
            # å³æ—¶æ˜¾ç¤ºç»“æœ
            print(f"åŸå§‹tokenæ•°: {result['original_length']}")
            print(f"TE1 77æˆªæ–­: {result['te1_77_length']} tokens, æˆªæ–­: {'æ˜¯' if result['te1_truncated'] else 'å¦'}")
            print(f"TE2 77æˆªæ–­: {result['te2_77_length']} tokens, æˆªæ–­: {'æ˜¯' if result['te2_truncated'] else 'å¦'}")
            print(f"TE1 ä¿¡æ¯ä¿ç•™: {te1_similarity:.4f} (æŸå¤±: {result['te1_info_loss']:.2f}%)")
            print(f"TE2 ä¿¡æ¯ä¿ç•™: {te2_similarity:.4f} (æŸå¤±: {result['te2_info_loss']:.2f}%)")
        
        # ç”Ÿæˆæ±‡æ€»è¡¨æ ¼
        print("\n" + "="*60)
        print("ğŸ“Š å®Œæ•´å¯¹æ¯”ç»“æœæ±‡æ€»è¡¨")
        print("="*60)
        
        header = f"{'ID':<3} {'ç±»å‹':<12} {'åŸé•¿':<5} {'æˆªæ–­':<5} {'TE1ä¿ç•™':<8} {'TE1æŸå¤±':<8} {'TE2ä¿ç•™':<8} {'TE2æŸå¤±':<8}"
        print(header)
        print("-" * len(header))
        
        for r in results:
            truncated_indicator = "æ˜¯" if r['te1_truncated'] or r['te2_truncated'] else "å¦"
            print(f"{r['id']:<3} {r['name']:<12} {r['original_length']:<5} {truncated_indicator:<5} "
                  f"{r['te1_similarity']:.4f}   {r['te1_info_loss']:.2f}%    "
                  f"{r['te2_similarity']:.4f}   {r['te2_info_loss']:.2f}%")
        
        # ç»Ÿè®¡åˆ†æ
        print("\nğŸ“ˆ ç»Ÿè®¡åˆ†æ:")
        truncated_cases = [r for r in results if r['te1_truncated'] or r['te2_truncated']]
        print(f"éœ€è¦æˆªæ–­çš„æ ·æœ¬: {len(truncated_cases)}/{len(results)} ({len(truncated_cases)/len(results)*100:.1f}%)")
        
        if truncated_cases:
            avg_te1_loss = sum(r['te1_info_loss'] for r in truncated_cases) / len(truncated_cases)
            avg_te2_loss = sum(r['te2_info_loss'] for r in truncated_cases) / len(truncated_cases)
            print(f"æˆªæ–­æ ·æœ¬å¹³å‡ä¿¡æ¯æŸå¤± - TE1: {avg_te1_loss:.2f}%, TE2: {avg_te2_loss:.2f}%")
            
            max_te1_loss = max(r['te1_info_loss'] for r in truncated_cases)
            max_te2_loss = max(r['te2_info_loss'] for r in truncated_cases)
            print(f"æœ€å¤§ä¿¡æ¯æŸå¤± - TE1: {max_te1_loss:.2f}%, TE2: {max_te2_loss:.2f}%")
        
        print("\n" + "="*60)
        print("ğŸ¯ ç»“è®º:")
        print("="*60)
        print("1. SDXLçš„225åˆ†å—æœºåˆ¶èƒ½å¤Ÿæœ‰æ•ˆå¤„ç†é•¿æ–‡æœ¬ï¼Œé¿å…ä¿¡æ¯æˆªæ–­")
        print("2. ä¼ ç»Ÿ77æˆªæ–­åœ¨é•¿æ–‡æœ¬ä¸Šä¼šé€ æˆæ˜¾è‘—çš„ä¿¡æ¯æŸå¤±")
        print("3. åŒç¼–ç å™¨åœ¨225åˆ†å—ä¸‹èƒ½ä¿æŒæ›´å®Œæ•´çš„è¯­ä¹‰è¡¨ç¤º")
        print("4. åˆ†å—æœºåˆ¶æ˜¯SDXLå¤„ç†å¤æ‚æç¤ºè¯çš„å…³é”®æŠ€æœ¯")

    def simulate_real_sdxl_chunking(self, text):
        """çœŸæ­£æ¨¡æ‹ŸSDXLçš„3å—åˆ†ç»„å¤„ç†æœºåˆ¶"""
        # åˆ†åˆ«å¯¹ä¸¤ä¸ªç¼–ç å™¨è¿›è¡ŒtokenåŒ–
        te1_tokens = self.tokenize_text_te1(text)
        te2_tokens = self.tokenize_text_te2(text)
        
        # è®¡ç®—éœ€è¦çš„å—æ•°ï¼ˆæ¯å—75ä¸ªæœ‰æ•ˆtoken + 2ä¸ªç‰¹æ®Štokenï¼‰
        max_tokens_per_chunk = 75
        
        def process_chunks(tokens, model):
            if len(tokens) <= 77:
                # å•å—å¤„ç†
                padded_tokens = tokens + [0] * (77 - len(tokens))
                with torch.no_grad():
                    hidden_states = model(torch.tensor([padded_tokens]).to(self.device)).last_hidden_state
                return hidden_states
            else:
                # å¤šå—å¤„ç†
                chunk_hidden_states = []
                for i in range(0, len(tokens), max_tokens_per_chunk):
                    chunk_tokens = tokens[i:i + max_tokens_per_chunk]
                    # æ·»åŠ ç‰¹æ®Štokenå¹¶å¡«å……åˆ°77
                    if len(chunk_tokens) < 77:
                        chunk_tokens = chunk_tokens + [0] * (77 - len(chunk_tokens))
                    else:
                        chunk_tokens = chunk_tokens[:77]
                    
                    with torch.no_grad():
                        chunk_hidden = model(torch.tensor([chunk_tokens]).to(self.device)).last_hidden_state
                        chunk_hidden_states.append(chunk_hidden)
                
                # ç»„åˆå¤šå—çš„hidden statesï¼ˆå–å¹³å‡ï¼‰
                if len(chunk_hidden_states) > 1:
                    combined_hidden = torch.stack(chunk_hidden_states).mean(dim=0)
                else:
                    combined_hidden = chunk_hidden_states[0]
                return combined_hidden
        
        # åˆ†åˆ«å¤„ç†ä¸¤ä¸ªç¼–ç å™¨
        te1_hidden = process_chunks(te1_tokens, self.te1_model)
        te2_hidden = process_chunks(te2_tokens, self.te2_model)
        
        return te1_hidden, te2_hidden
    
    def mystery_2_shuffle_caption_mechanism(self):
        """ç–‘æƒ‘2: shuffle_captionçš„æ·±å±‚æœºåˆ¶éªŒè¯"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘2: shuffle_captionæœºåˆ¶çš„é‡åŒ–éªŒè¯")
        print("="*60)
        
        # æ¨¡æ‹Ÿshuffle_captionçš„æ•ˆæœ
        base_prompt = "ohwx, 1girl, beautiful, detailed eyes, long hair, smiling, standing, outdoor, sunlight"
        tags = base_prompt.split(", ")
        trigger_word = tags[0]  # "ohwx"
        other_tags = tags[1:]   # å…¶ä»–æ ‡ç­¾
        
        print(f"åŸºç¡€æç¤º: {base_prompt}")
        print(f"è§¦å‘è¯: {trigger_word}")
        print(f"å¯æ‰“ä¹±æ ‡ç­¾: {other_tags}")
        
        # ç”Ÿæˆå¤šä¸ªæ‰“ä¹±ç‰ˆæœ¬
        shuffled_versions = []
        for i in range(10):  # ç”Ÿæˆ10ä¸ªä¸åŒçš„æ‰“ä¹±ç‰ˆæœ¬
            shuffled_tags = other_tags.copy()
            random.shuffle(shuffled_tags)
            shuffled_prompt = trigger_word + ", " + ", ".join(shuffled_tags)
            shuffled_versions.append(shuffled_prompt)
        
        # æµ‹è¯•æ¦‚å¿µä¸€è‡´æ€§
        print(f"\nğŸ“Š æ‰“ä¹±æ•ˆæœåˆ†æ:")
        
        original_embeddings = {}
        shuffled_embeddings = {}
        
        for model_name in ["TE1", "TE2"]:
            # ç¼–ç åŸå§‹ç‰ˆæœ¬
            original_result = self.encode_text_with_details(base_prompt, model_name)
            original_embeddings[model_name] = original_result['embedding']
            
            # ç¼–ç æ‰“ä¹±ç‰ˆæœ¬
            shuffled_embeddings[model_name] = []
            similarities = []
            
            for i, shuffled_prompt in enumerate(shuffled_versions):
                shuffled_result = self.encode_text_with_details(shuffled_prompt, model_name)
                shuffled_embeddings[model_name].append(shuffled_result['embedding'])
                
                # è®¡ç®—ä¸åŸå§‹ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦
                similarity = cosine_similarity(original_embeddings[model_name], 
                                             shuffled_result['embedding'])[0][0]
                similarities.append(similarity)
                
                if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªè¯¦ç»†ä¿¡æ¯
                    print(f"  {model_name} ç‰ˆæœ¬{i+1}: {similarity:.4f}")
                    print(f"    åŸå§‹: {base_prompt[:60]}...")
                    print(f"    æ‰“ä¹±: {shuffled_prompt[:60]}...")
            
            avg_similarity = np.mean(similarities)
            std_similarity = np.std(similarities)
            
            print(f"\n  {model_name} ç»Ÿè®¡:")
            print(f"    å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} Â± {std_similarity:.4f}")
            print(f"    ä¸€è‡´æ€§è¯„ä»·: {'ä¼˜ç§€' if avg_similarity > 0.95 else 'è‰¯å¥½' if avg_similarity > 0.90 else 'éœ€è¦æ”¹è¿›'}")
        
        # åˆ†æè§¦å‘è¯ä½ç½®çš„é‡è¦æ€§
        print(f"\nğŸ¯ è§¦å‘è¯ä½ç½®é‡è¦æ€§åˆ†æ:")
        
        # æµ‹è¯•è§¦å‘è¯åœ¨ä¸åŒä½ç½®çš„æ•ˆæœ
        position_tests = [
            f"{trigger_word}, " + ", ".join(other_tags),  # å¼€å¤´
            ", ".join(other_tags[:3]) + f", {trigger_word}, " + ", ".join(other_tags[3:]),  # ä¸­é—´
            ", ".join(other_tags) + f", {trigger_word}"   # ç»“å°¾
        ]
        
        position_names = ["å¼€å¤´", "ä¸­é—´", "ç»“å°¾"]
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} ä½ç½®æµ‹è¯•:")
            baseline_emb = original_embeddings[model_name]
            
            for pos_name, test_prompt in zip(position_names, position_tests):
                result = self.encode_text_with_details(test_prompt, model_name)
                similarity = cosine_similarity(baseline_emb, result['embedding'])[0][0]
                print(f"    è§¦å‘è¯åœ¨{pos_name}: {similarity:.4f}")
        
        return {
            'original_embeddings': original_embeddings,
            'shuffled_embeddings': shuffled_embeddings,
            'shuffled_versions': shuffled_versions
        }
    
    def mystery_3_structured_format_optimization(self):
        """ç–‘æƒ‘3: ç»“æ„åŒ–æ‹¼æ¥çš„æœ€ä¼˜æ ¼å¼æ¢ç´¢"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘3: ç»“æ„åŒ–æ‹¼æ¥çš„æœ€ä¼˜æ ¼å¼æ¢ç´¢") 
        print("="*60)
        
        # åŠ è½½çœŸå®æ•°æ®æ ·æœ¬
        data_path = Path("/root/data/cluster_4")
        txt_files = list(data_path.glob("*.txt"))
        
        if not txt_files:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶")
            return None
        
        print(f"ğŸ“Š å‘ç° {len(txt_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå°†è¿›è¡Œå…¨é‡åˆ†æ")
        
        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
        samples = []
        print("ğŸ”„ åŠ è½½æ•°æ®æ ·æœ¬...")
        
        for txt_file in tqdm(txt_files, desc="è¯»å–æ–‡ä»¶"):
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    lines = content.split('\n')
                    
                    if len(lines) >= 2 and lines[0].strip() and lines[1].strip():
                        samples.append({
                            'filename': txt_file.name,
                            'tags': lines[0].strip(),
                            'description': lines[1].strip()
                        })
            except Exception as e:
                continue  # è·³è¿‡æœ‰é—®é¢˜çš„æ–‡ä»¶
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        if len(samples) < 100:
            print("âŒ æœ‰æ•ˆæ ·æœ¬æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œå¯é åˆ†æ")
            return None
        
        print(f"ğŸ¯ å°†å¯¹å…¨éƒ¨ {len(samples)} ä¸ªæ ·æœ¬è¿›è¡Œå®Œæ•´çš„æ ¼å¼åˆ†æ")
        
        # å®šä¹‰ä¸åŒçš„ç»“æ„åŒ–æ ¼å¼
        def create_format_templates(tags, desc):
            return {
                "æ–¹æ¡ˆ1-ç®€å•æ‹¼æ¥": f"{tags}, {desc}",
                "æ–¹æ¡ˆ2-å†’å·åˆ†éš”": f"tags: {tags}. description: {desc}",
                "æ–¹æ¡ˆ3-ç«–çº¿åˆ†éš”": f"keywords: {tags} | scene: {desc}",
                "æ–¹æ¡ˆ4-æ‹¬å·ç»“æ„": f"[tags: {tags}] [description: {desc}]",
                "æ–¹æ¡ˆ5-æ˜ç¡®æ ‡è¯†": f"TAGS: {tags}. DESCRIPTION: {desc}",
                "æ–¹æ¡ˆ6-è‡ªç„¶è¯­è¨€": f"This image contains {tags}. The scene shows {desc}",
                "æ–¹æ¡ˆ7-JSONé£æ ¼": f"{{tags: {tags}, description: {desc}}}",
                "æ–¹æ¡ˆ8-æ¢è¡Œåˆ†éš”": f"{tags}\n{desc}",
            }
        
        print(f"\nğŸ“Š ç»“æ„åŒ–æ ¼å¼æ•ˆæœåˆ†æ:")
        
        format_results = {}
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} å…¨é‡æµ‹è¯•ç»“æœ:")
            format_results[model_name] = {}
            
            # ä¸ºæ¯ç§æ ¼å¼æ”¶é›†ç»Ÿè®¡æ•°æ®
            format_stats = {}
            
            # é¦–å…ˆç¼–ç åŸºå‡†æ•°æ®ï¼ˆä»…æ ‡ç­¾å’Œä»…æè¿°ï¼‰
            print(f"    ç¼–ç åŸºå‡†æ•°æ® (å…¨éƒ¨{len(samples)}ä¸ªæ ·æœ¬)...")
            tags_list = [sample['tags'] for sample in samples]
            desc_list = [sample['description'] for sample in samples]
            
            tags_embeddings, _ = self.encode_batch_texts(tags_list, model_name, batch_size=128)
            desc_embeddings, _ = self.encode_batch_texts(desc_list, model_name, batch_size=128)
            
            # æµ‹è¯•æ¯ç§æ ¼å¼
            format_names = ["æ–¹æ¡ˆ1-ç®€å•æ‹¼æ¥", "æ–¹æ¡ˆ2-å†’å·åˆ†éš”", "æ–¹æ¡ˆ3-ç«–çº¿åˆ†éš”", "æ–¹æ¡ˆ4-æ‹¬å·ç»“æ„", 
                          "æ–¹æ¡ˆ5-æ˜ç¡®æ ‡è¯†", "æ–¹æ¡ˆ6-è‡ªç„¶è¯­è¨€", "æ–¹æ¡ˆ7-JSONé£æ ¼", "æ–¹æ¡ˆ8-æ¢è¡Œåˆ†éš”"]
            
            for format_name in format_names:
                print(f"    æµ‹è¯• {format_name} (å…¨éƒ¨{len(samples)}ä¸ªæ ·æœ¬)...")
                
                # ç”Ÿæˆè¯¥æ ¼å¼çš„æ‰€æœ‰æ–‡æœ¬
                format_texts = []
                for sample in samples:
                    templates = create_format_templates(sample['tags'], sample['description'])
                    format_texts.append(templates[format_name])
                
                # æ‰¹é‡ç¼–ç 
                format_embeddings, format_details = self.encode_batch_texts(format_texts, model_name, batch_size=128)
                
                # è®¡ç®—ä¸åŸºå‡†çš„ç›¸ä¼¼åº¦
                similarities_tags = []
                similarities_desc = []
                token_counts = []
                truncation_rates = []
                
                for i in range(len(format_embeddings)):
                    # ä¸æ ‡ç­¾çš„ç›¸ä¼¼åº¦
                    sim_tag = cosine_similarity(tags_embeddings[i:i+1], format_embeddings[i:i+1])[0][0]
                    similarities_tags.append(sim_tag)
                    
                    # ä¸æè¿°çš„ç›¸ä¼¼åº¦
                    sim_desc = cosine_similarity(desc_embeddings[i:i+1], format_embeddings[i:i+1])[0][0]
                    similarities_desc.append(sim_desc)
                    
                    # Tokenç»Ÿè®¡
                    token_counts.append(format_details[i]['valid_tokens'])
                    truncation_rates.append(1 if format_details[i]['is_truncated'] else 0)
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                avg_sim_tags = np.mean(similarities_tags)
                avg_sim_desc = np.mean(similarities_desc)
                balance_score = min(avg_sim_tags, avg_sim_desc)  # å¹³è¡¡åˆ†æ•°
                avg_tokens = np.mean(token_counts)
                truncation_rate = np.mean(truncation_rates)
                
                # è®¡ç®—æ ‡å‡†å·®å’Œå…¶ä»–ç»Ÿè®¡æŒ‡æ ‡
                std_sim_tags = np.std(similarities_tags)
                std_sim_desc = np.std(similarities_desc)
                median_sim_tags = np.median(similarities_tags)
                median_sim_desc = np.median(similarities_desc)
                
                format_stats[format_name] = {
                    'avg_sim_tags': avg_sim_tags,
                    'avg_sim_desc': avg_sim_desc,
                    'std_sim_tags': std_sim_tags,
                    'std_sim_desc': std_sim_desc,
                    'median_sim_tags': median_sim_tags,
                    'median_sim_desc': median_sim_desc,
                    'balance_score': balance_score,
                    'avg_tokens': avg_tokens,
                    'truncation_rate': truncation_rate,
                    'sample_count': len(samples)
                }
                
                print(f"      ä¸æ ‡ç­¾å¹³å‡ç›¸ä¼¼åº¦: {avg_sim_tags:.4f} Â± {std_sim_tags:.4f}")
                print(f"      ä¸æè¿°å¹³å‡ç›¸ä¼¼åº¦: {avg_sim_desc:.4f} Â± {std_sim_desc:.4f}")
                print(f"      å¹³è¡¡åˆ†æ•°: {balance_score:.4f}")
                print(f"      å¹³å‡tokenæ•°: {avg_tokens:.1f}")
                print(f"      æˆªæ–­ç‡: {truncation_rate:.2%}")
                print(f"      æ ·æœ¬æ•°é‡: {len(samples)}")
                
                # æ¸…ç†å†…å­˜
                del format_embeddings
                gc.collect()
                torch.cuda.empty_cache()
            
            format_results[model_name] = format_stats
            
            # æ‰¾å‡ºæœ€ä½³æ ¼å¼
            best_format = max(format_stats.keys(), key=lambda x: format_stats[x]['balance_score'])
            best_score = format_stats[best_format]['balance_score']
            
            print(f"\n    ğŸ† {model_name} æœ€ä½³æ ¼å¼: {best_format}")
            print(f"       å¹³è¡¡åˆ†æ•°: {best_score:.4f}")
            print(f"       æ ·æœ¬æ•°é‡: {format_stats[best_format]['sample_count']}")
        
        # è·¨æ¨¡å‹åˆ†æ
        print(f"\nğŸ¯ è·¨æ¨¡å‹æ ¼å¼ä¸€è‡´æ€§åˆ†æ (åŸºäºå…¨éƒ¨{len(samples)}ä¸ªæ ·æœ¬):")
        consistency_scores = {}
        
        for format_name in format_names:
            te1_score = format_results["TE1"][format_name]['balance_score']
            te2_score = format_results["TE2"][format_name]['balance_score']
            consistency = 1 - abs(te1_score - te2_score)  # ä¸€è‡´æ€§åˆ†æ•°
            consistency_scores[format_name] = consistency
            
            print(f"  {format_name}:")
            print(f"    TE1å¹³è¡¡åˆ†æ•°: {te1_score:.4f}")
            print(f"    TE2å¹³è¡¡åˆ†æ•°: {te2_score:.4f}")
            print(f"    è·¨æ¨¡å‹ä¸€è‡´æ€§: {consistency:.4f}")
            print(f"    åŸºäºæ ·æœ¬æ•°: {len(samples)}")
        
        # æ¨èæœ€ä½³æ ¼å¼
        best_consistency = max(consistency_scores.keys(), key=lambda x: consistency_scores[x])
        print(f"\nğŸŒŸ è·¨æ¨¡å‹ä¸€è‡´æ€§æœ€ä½³æ ¼å¼: {best_consistency}")
        print(f"   ä¸€è‡´æ€§åˆ†æ•°: {consistency_scores[best_consistency]:.4f}")
        print(f"   åŸºäºå…¨é‡æ•°æ®: {len(samples)} ä¸ªæ ·æœ¬")
        
        return format_results
    
    def mystery_4_overfitting_detection(self):
        """ç–‘æƒ‘4: è¿‡æ‹Ÿåˆæ£€æµ‹çš„å®šé‡æŒ‡æ ‡"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘4: åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„è¿‡æ‹Ÿåˆæ£€æµ‹æ–¹æ³•")
        print("="*60)
        
        # åŠ è½½å…¨éƒ¨æ•°æ®æ ·æœ¬
        data_path = Path("/root/data/cluster_4")
        txt_files = list(data_path.glob("*.txt"))
        
        if len(txt_files) < 100:
            print("âŒ æ•°æ®æ ·æœ¬æ•°é‡ä¸è¶³")
            return None
        
        print(f"ğŸ“Š å‘ç° {len(txt_files)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œå°†è¿›è¡Œå…¨é‡è¿‡æ‹Ÿåˆæ£€æµ‹åˆ†æ")
        
        # åŠ è½½æ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
        samples = []
        print("ğŸ”„ åŠ è½½æ•°æ®æ ·æœ¬...")
        
        for txt_file in tqdm(txt_files, desc="è¯»å–æ–‡ä»¶"):
            try:
                with open(txt_file, 'r', encoding='utf-8') as f:
                    content = f.read().strip()
                    lines = content.split('\n')
                    if len(lines) >= 2 and lines[0].strip() and lines[1].strip():
                        samples.append({
                            'filename': txt_file.name,
                            'tags': lines[0].strip(),
                            'description': lines[1].strip(),
                            'combined': f"{lines[0].strip()}, {lines[1].strip()}"
                        })
            except Exception as e:
                continue
        
        print(f"âœ… æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        # ä¸ºäº†å†…å­˜æ•ˆç‡ï¼Œåˆ†æ‰¹å¤„ç†å¤§æ•°æ®é›†
        batch_size = 1000  # æ¯æ‰¹å¤„ç†1000ä¸ªæ ·æœ¬
        overfitting_metrics = {}
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} å…¨é‡è¿‡æ‹Ÿåˆæ£€æµ‹åˆ†æ:")
            
            all_similarities = []
            processed_samples = 0
            
            # åˆ†æ‰¹å¤„ç†
            for batch_start in tqdm(range(0, len(samples), batch_size), desc=f"{model_name} æ‰¹å¤„ç†"):
                batch_end = min(batch_start + batch_size, len(samples))
                batch_samples = samples[batch_start:batch_end]
                
                # æ‰¹é‡ç¼–ç 
                batch_texts = [sample['combined'] for sample in batch_samples]
                batch_embeddings, _ = self.encode_batch_texts(batch_texts, model_name, batch_size=32)
                
                # è®¡ç®—æ‰¹å†…ç›¸ä¼¼åº¦
                if len(batch_embeddings) > 1:
                    batch_similarity_matrix = cosine_similarity(batch_embeddings)
                    
                    # æå–ä¸Šä¸‰è§’çŸ©é˜µï¼ˆæ’é™¤å¯¹è§’çº¿ï¼‰
                    n = len(batch_similarity_matrix)
                    for i in range(n):
                        for j in range(i+1, n):
                            all_similarities.append(batch_similarity_matrix[i][j])
                
                processed_samples += len(batch_samples)
                
                # æ¸…ç†å†…å­˜
                del batch_embeddings
                gc.collect()
                torch.cuda.empty_cache()
                
                # æ˜¾ç¤ºè¿›åº¦
                if batch_start % (batch_size * 5) == 0:  # æ¯5æ‰¹æ˜¾ç¤ºä¸€æ¬¡
                    current_avg = np.mean(all_similarities) if all_similarities else 0
                    print(f"    å·²å¤„ç† {processed_samples}/{len(samples)} æ ·æœ¬ï¼Œå½“å‰å¹³å‡ç›¸ä¼¼åº¦: {current_avg:.4f}")
            
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡æŒ‡æ ‡
            if all_similarities:
                metrics = self.calculate_comprehensive_overfitting_metrics(all_similarities, len(samples))
                overfitting_metrics[model_name] = metrics
                
                print(f"\n    ğŸ“Š {model_name} å…¨é‡ç»Ÿè®¡ç»“æœ (åŸºäº{len(samples)}ä¸ªæ ·æœ¬):")
                print(f"      æ ·æœ¬å¯¹æ•°é‡: {len(all_similarities):,}")
                print(f"      å¹³å‡æ ·æœ¬é—´ç›¸ä¼¼åº¦: {metrics['avg_similarity']:.4f}")
                print(f"      ç›¸ä¼¼åº¦æ ‡å‡†å·®: {metrics['similarity_std']:.4f}")
                print(f"      ç›¸ä¼¼åº¦ä¸­ä½æ•°: {metrics['median_similarity']:.4f}")
                print(f"      æœ€é«˜ç›¸ä¼¼åº¦: {metrics['max_similarity']:.4f}")
                print(f"      æœ€ä½ç›¸ä¼¼åº¦: {metrics['min_similarity']:.4f}")
                print(f"      å¤šæ ·æ€§æŒ‡æ•°: {metrics['diversity_index']:.4f}")
                print(f"      é«˜ç›¸ä¼¼åº¦æ¯”ä¾‹ (>0.9): {metrics['high_similarity_ratio']:.2%}")
                print(f"      è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°: {metrics['overfitting_risk']}")
            else:
                print(f"    âŒ {model_name} æ— æ³•è®¡ç®—ç›¸ä¼¼åº¦ç»Ÿè®¡")
        
        # å»ºè®®è¿‡æ‹Ÿåˆæ£€æµ‹é˜ˆå€¼ï¼ˆåŸºäºå…¨é‡æ•°æ®ï¼‰
        print(f"\nğŸ“‹ åŸºäºå…¨é‡æ•°æ®çš„è¿‡æ‹Ÿåˆæ£€æµ‹å»ºè®®:")
        print(f"  1. å¹³å‡æ ·æœ¬é—´ç›¸ä¼¼åº¦ > 0.80: é«˜é£é™© (å…¨é‡æ•°æ®åŸºå‡†)")
        print(f"  2. ç›¸ä¼¼åº¦æ ‡å‡†å·® < 0.08: ç¼ºä¹å¤šæ ·æ€§")
        print(f"  3. é«˜ç›¸ä¼¼åº¦æ¯”ä¾‹ > 30%: æ¦‚å¿µé‡å¤é£é™©")
        print(f"  4. å¤šæ ·æ€§æŒ‡æ•° < 0.15: æ¦‚å¿µåå¡Œé£é™©")
        
        # æä¾›è®­ç»ƒå»ºè®®
        for model_name in ["TE1", "TE2"]:
            if model_name in overfitting_metrics:
                metrics = overfitting_metrics[model_name]
                print(f"\n  {model_name} è®­ç»ƒå»ºè®® (åŸºäº{len(samples)}æ ·æœ¬åˆ†æ):")
                
                if metrics['avg_similarity'] > 0.80:
                    print(f"    âš ï¸  å¹³å‡ç›¸ä¼¼åº¦è¿‡é«˜ ({metrics['avg_similarity']:.4f})ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ•°æ®å¤šæ ·æ€§")
                
                if metrics['similarity_std'] < 0.08:
                    print(f"    âš ï¸  å¤šæ ·æ€§ä¸è¶³ (std={metrics['similarity_std']:.4f})ï¼Œå»ºè®®æ·»åŠ æ›´å¤šä¸åŒé£æ ¼çš„è®­ç»ƒæ•°æ®")
                
                if metrics['high_similarity_ratio'] > 0.3:
                    print(f"    âš ï¸  é«˜ç›¸ä¼¼åº¦æ ·æœ¬è¿‡å¤š ({metrics['high_similarity_ratio']:.2%})ï¼Œå­˜åœ¨æ•°æ®é‡å¤é£é™©")
                
                if metrics['overfitting_risk'] == "é«˜":
                    print(f"    ğŸš¨ é«˜è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®ç«‹å³åœæ­¢è®­ç»ƒæˆ–å›é€€åˆ°ä¹‹å‰çš„checkpoint")
                elif metrics['overfitting_risk'] == "ä¸­":
                    print(f"    âš ï¸  ä¸­ç­‰é£é™©ï¼Œå»ºè®®å¯†åˆ‡ç›‘æ§åç»­è®­ç»ƒ")
                else:
                    print(f"    âœ… é£é™©è¾ƒä½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
        
        return overfitting_metrics
    
    def calculate_comprehensive_overfitting_metrics(self, similarities, total_samples):
        """è®¡ç®—åŸºäºå…¨é‡æ•°æ®çš„è¿‡æ‹Ÿåˆæ£€æµ‹æŒ‡æ ‡"""
        similarities = np.array(similarities)
        
        avg_similarity = np.mean(similarities)
        similarity_std = np.std(similarities)
        median_similarity = np.median(similarities)
        max_similarity = np.max(similarities)
        min_similarity = np.min(similarities)
        
        # å¤šæ ·æ€§æŒ‡æ•°ï¼šæ ‡å‡†å·®é™¤ä»¥å¹³å‡å€¼
        diversity_index = similarity_std / avg_similarity if avg_similarity > 0 else 0
        
        # é«˜ç›¸ä¼¼åº¦æ¯”ä¾‹
        high_similarity_ratio = np.sum(similarities > 0.9) / len(similarities)
        
        # è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°ï¼ˆåŸºäºå…¨é‡æ•°æ®è°ƒæ•´é˜ˆå€¼ï¼‰
        risk_score = 0
        if avg_similarity > 0.80:  # å…¨é‡æ•°æ®é˜ˆå€¼è°ƒæ•´
            risk_score += 2
        elif avg_similarity > 0.70:
            risk_score += 1
            
        if similarity_std < 0.08:  # å…¨é‡æ•°æ®é˜ˆå€¼è°ƒæ•´
            risk_score += 2
        elif similarity_std < 0.12:
            risk_score += 1
            
        if high_similarity_ratio > 0.3:
            risk_score += 2
        elif high_similarity_ratio > 0.2:
            risk_score += 1
        
        if max_similarity > 0.98:
            risk_score += 1
        
        if risk_score >= 5:
            overfitting_risk = "é«˜"
        elif risk_score >= 3:
            overfitting_risk = "ä¸­"
        else:
            overfitting_risk = "ä½"
        
        return {
            'avg_similarity': avg_similarity,
            'similarity_std': similarity_std,
            'median_similarity': median_similarity,
            'max_similarity': max_similarity,
            'min_similarity': min_similarity,
            'diversity_index': diversity_index,
            'high_similarity_ratio': high_similarity_ratio,
            'overfitting_risk': overfitting_risk,
            'risk_score': risk_score,
            'total_samples': total_samples,
            'total_comparisons': len(similarities)
        }

def main():
    print("ğŸš€ å¯åŠ¨SDXLè®­ç»ƒç–‘æƒ‘ç»¼åˆè§£è°œåˆ†æ - å…¨é‡æ•°æ®ç‰ˆæœ¬")
    print("="*60)
    
    analyzer = MysteryAnalyzer()
    
    # æ‰§è¡Œæ‰€æœ‰åˆ†æ
    all_results = {}
    
    try:
        # ç–‘æƒ‘1: æ–‡æœ¬é•¿åº¦å½±å“
        all_results['length_impact'] = analyzer.mystery_1_text_length_impact()
        
        # ç–‘æƒ‘2: shuffle_captionæœºåˆ¶
        all_results['shuffle_mechanism'] = analyzer.mystery_2_shuffle_caption_mechanism()
        
        # ç–‘æƒ‘3: ç»“æ„åŒ–æ ¼å¼ä¼˜åŒ– (å…¨é‡æ•°æ®)
        all_results['structured_format'] = analyzer.mystery_3_structured_format_optimization()
        
        # ç–‘æƒ‘4: è¿‡æ‹Ÿåˆæ£€æµ‹ (å…¨é‡æ•°æ®)
        all_results['overfitting_detection'] = analyzer.mystery_4_overfitting_detection()
        
        # ä¿å­˜ç»“æœ
        with open('/root/mystery_analysis_results_full.json', 'w', encoding='utf-8') as f:
            # åªä¿å­˜å¯åºåˆ—åŒ–çš„æ•°æ®
            serializable_results = {}
            for key, value in all_results.items():
                if key == 'length_impact':
                    serializable_results[key] = [
                        {k: v for k, v in item.items() if k != 'embedding'}
                        for item in value
                    ]
                elif key in ['structured_format', 'overfitting_detection']:
                    if value is not None:
                        serializable_results[key] = {
                            k: {k2: {k3: v3 for k3, v3 in v2.items() if k3 not in ['embedding']} 
                                if isinstance(v2, dict) else v2 
                                for k2, v2 in v.items()} 
                            if isinstance(v, dict) else v 
                            for k, v in value.items()
                        }
                else:
                    serializable_results[key] = value
            
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ å…¨é‡æ•°æ®åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° /root/mystery_analysis_results_full.json")
        
        # è¾“å‡ºæ€»ç»“
        print(f"\n" + "="*60)
        print("ğŸ“ åŸºäºå…¨é‡æ•°æ®çš„æ ¸å¿ƒå‘ç°æ€»ç»“:")
        print("="*60)
        print("1. æ–‡æœ¬é•¿åº¦æˆªæ–­ç¡®å®ä¼šå½±å“è¯­ä¹‰ç†è§£ï¼Œä½†å½±å“ç¨‹åº¦å› æ¨¡å‹è€Œå¼‚")
        print("2. shuffle_captionæœºåˆ¶åœ¨ç»´æŒæ¦‚å¿µä¸€è‡´æ€§æ–¹é¢è¡¨ç°è‰¯å¥½")  
        print("3. ç»“æ„åŒ–æ‹¼æ¥æ ¼å¼çš„é€‰æ‹©å¯¹åŒç¼–ç å™¨æ€§èƒ½æœ‰æ˜¾è‘—å½±å“ (åŸºäºå…¨é‡æ•°æ®éªŒè¯)")
        print("4. åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„è¿‡æ‹Ÿåˆæ£€æµ‹æ˜¯å¯è¡Œçš„ç›‘æ§æ–¹æ³• (å…¨é‡æ•°æ®éªŒè¯)")
        print("5. å…¨é‡æ•°æ®åˆ†ææä¾›äº†æ›´å¯é çš„ç»Ÿè®¡åŸºå‡†å’Œé˜ˆå€¼å»ºè®®")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 