#!/usr/bin/env python3
"""
SDXLè®­ç»ƒç–‘æƒ‘ç»¼åˆè§£è°œè„šæœ¬
åŸºäºå®éªŒéªŒè¯æ–‡æ¡£ä¸­æå‡ºä½†æœªå®Œå…¨è§£å†³çš„å…³é”®é—®é¢˜
"""

import torch
import numpy as np
from transformers import CLIPTokenizer, CLIPTextModel
from pathlib import Path
from sklearn.metrics.pairwise import cosine_similarity
import json
import random
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import time

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class MysteryAnalyzer:
    def __init__(self):
        self.models = {}
        self.tokenizers = {}
        self.load_models()
        
    def load_models(self):
        """åŠ è½½TE1å’ŒTE2æ¨¡å‹"""
        model_configs = {
            "TE1": "/root/text_encoder/CLIP-ViT-B-32-laion2B-s34B-b79K",
            "TE2": "/root/text_encoder/clip-vit-large-patch14"
        }
        
        for name, path in model_configs.items():
            print(f"åŠ è½½ {name} æ¨¡å‹...")
            self.tokenizers[name] = CLIPTokenizer.from_pretrained(path)
            self.models[name] = CLIPTextModel.from_pretrained(path).to(device)
            self.models[name].eval()
        
        print(f"æ‰€æœ‰æ¨¡å‹å·²åŠ è½½åˆ° {device}")
    
    def encode_text_with_details(self, text, model_name="TE1", max_length=77):
        """è¯¦ç»†ç¼–ç æ–‡æœ¬ï¼Œè¿”å›åµŒå…¥å’Œtokenä¿¡æ¯"""
        tokenizer = self.tokenizers[model_name]
        model = self.models[model_name]
        
        # åˆ†è¯
        tokens = tokenizer(text, truncation=True, padding="max_length", 
                          max_length=max_length, return_tensors="pt")
        
        # è®¡ç®—æœ‰æ•ˆtokenæ•°é‡
        input_ids = tokens['input_ids'][0]
        valid_tokens = (input_ids != tokenizer.pad_token_id).sum().item()
        
        # æ£€æŸ¥æ˜¯å¦è¢«æˆªæ–­
        original_tokens = tokenizer(text, truncation=False)['input_ids']
        is_truncated = len(original_tokens) > max_length
        
        # ç¼–ç 
        with torch.no_grad():
            tokens = tokens.to(device)
            outputs = model(**tokens)
            embedding = outputs.last_hidden_state.mean(dim=1).cpu().numpy()
        
        return {
            'embedding': embedding,
            'valid_tokens': valid_tokens,
            'is_truncated': is_truncated,
            'original_length': len(original_tokens),
            'tokens_preview': tokenizer.decode(input_ids[:10]) if valid_tokens > 0 else ""
        }
    
    def mystery_1_text_length_impact(self):
        """ç–‘æƒ‘1: æ–‡æœ¬é•¿åº¦æˆªæ–­å¯¹åŒç¼–ç å™¨æ€§èƒ½çš„å®é™…å½±å“"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘1: æ–‡æœ¬é•¿åº¦æˆªæ–­å¯¹åŒç¼–ç å™¨æ€§èƒ½çš„å®é™…å½±å“")
        print("="*60)
        
        # å‡†å¤‡ä¸åŒé•¿åº¦çš„æµ‹è¯•æ ·æœ¬
        test_cases = [
            # çŸ­æ–‡æœ¬ (< 30 tokens)
            "1girl, solo, beautiful, detailed eyes",
            
            # ä¸­ç­‰é•¿åº¦ (30-50 tokens)
            "1girl, solo, beautiful detailed eyes, long flowing hair, elegant dress, standing in garden, sunlight, masterpiece",
            
            # æ¥è¿‘é™åˆ¶é•¿åº¦ (50-77 tokens)  
            "1girl, solo, beautiful detailed sparkling blue eyes, long flowing golden hair with intricate braids, elegant white victorian dress with lace details, standing in a magnificent rose garden, warm golden sunlight filtering through leaves, masterpiece, best quality, ultra detailed",
            
            # è¶…é•¿æ–‡æœ¬ (> 77 tokensï¼Œä¼šè¢«æˆªæ–­)
            "1girl, solo, beautiful detailed sparkling blue eyes with long eyelashes, long flowing golden hair with intricate braids and small flowers, elegant white victorian dress with delicate lace details and pearl buttons, standing gracefully in a magnificent rose garden with red and pink roses, warm golden sunlight filtering through green leaves creating beautiful shadows, masterpiece, best quality, ultra detailed, photorealistic, 8k resolution, professional photography, perfect composition, cinematic lighting, highly detailed background"
        ]
        
        results = []
        
        print("âš ï¸  é‡è¦å‘ç°: CLIPæ¨¡å‹æœ€å¤§æ”¯æŒ77ä¸ªtokenä½ç½®ï¼Œè¶…å‡ºéƒ¨åˆ†ä¼šè¢«æˆªæ–­")
        print("è¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆæ–‡æ¡£å»ºè®®ä½¿ç”¨ --max_token_length=225 çš„å›°æƒ‘\n")
        
        for i, text in enumerate(test_cases):
            print(f"\næµ‹è¯•æ ·æœ¬ {i+1}:")
            print(f"åŸæ–‡: {text[:100]}{'...' if len(text) > 100 else ''}")
            
            for model_name in ["TE1", "TE2"]:
                # åªæµ‹è¯•77é•¿åº¦é™åˆ¶ï¼Œå› ä¸ºè¿™æ˜¯CLIPçš„ç¡¬é™åˆ¶
                result = self.encode_text_with_details(text, model_name, max_length=77)
                
                result_entry = {
                    'sample_id': i+1,
                    'model': model_name,
                    'max_length': 77,
                    'original_text_length': len(text),
                    'original_tokens': result['original_length'],
                    'used_tokens': result['valid_tokens'],
                    'is_truncated': result['is_truncated'],
                    'truncation_loss_rate': (result['original_length'] - result['valid_tokens']) / result['original_length'] if result['original_length'] > 0 else 0,
                    'embedding': result['embedding']
                }
                results.append(result_entry)
                
                print(f"  {model_name}: {result['valid_tokens']}/{result['original_length']} tokens, "
                      f"æˆªæ–­: {'æ˜¯' if result['is_truncated'] else 'å¦'}, "
                      f"æŸå¤±ç‡: {result_entry['truncation_loss_rate']:.2%}")
        
        # åˆ†ææˆªæ–­å¯¹è¯­ä¹‰ç†è§£çš„å½±å“
        print(f"\nğŸ“Š æˆªæ–­å½±å“åˆ†æ:")
        
        # å°†è¶…é•¿æ–‡æœ¬æ‰‹åŠ¨æˆªæ–­ï¼Œæ¯”è¾ƒæˆªæ–­å‰åçš„è¯­ä¹‰ç›¸ä¼¼åº¦
        long_text = test_cases[3]  # è¶…é•¿æ–‡æœ¬
        
        for model_name in ["TE1", "TE2"]:
            # è·å–å®Œæ•´åµŒå…¥ï¼ˆå®é™…ä¸Šå·²ç»è¢«æˆªæ–­åˆ°77ï¼‰
            full_result = self.encode_text_with_details(long_text, model_name, max_length=77)
            
            # æ‰‹åŠ¨æˆªæ–­åˆ°ä¸åŒé•¿åº¦è¿›è¡Œå¯¹æ¯”
            tokenizer = self.tokenizers[model_name]
            tokens = tokenizer(long_text, truncation=False)['input_ids']
            
            # æµ‹è¯•ä¸åŒæˆªæ–­ç‚¹çš„æ•ˆæœ
            for cut_length in [30, 45, 60, 77]:
                if len(tokens) > cut_length:
                    # æ‰‹åŠ¨æˆªæ–­
                    cut_tokens = tokens[:cut_length-1] + [tokens[-1]]  # ä¿ç•™ç»“æŸtoken
                    cut_text = tokenizer.decode(cut_tokens, skip_special_tokens=True)
                    
                    cut_result = self.encode_text_with_details(cut_text, model_name, max_length=77)
                    
                    # è®¡ç®—ä¸åŸå§‹ï¼ˆ77æˆªæ–­ï¼‰ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦
                    similarity = cosine_similarity(full_result['embedding'], cut_result['embedding'])[0][0]
                    
                    print(f"  {model_name} æˆªæ–­åˆ°{cut_length}tokens: ç›¸ä¼¼åº¦ = {similarity:.4f}")
        
        return results
    
    def mystery_2_shuffle_caption_mechanism(self):
        """ç–‘æƒ‘2: shuffle_captionçš„æ·±å±‚æœºåˆ¶éªŒè¯"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘2: shuffle_captionæœºåˆ¶çš„é‡åŒ–éªŒè¯")
        print("="*60)
        
        # æ¨¡æ‹Ÿshuffle_captionçš„æ•ˆæœ
        base_prompt = "ohwx, 1girl, beautiful, detailed eyes, long hair, smiling, standing, outdoor, sunlight"
        tags = base_prompt.split(", ")
        trigger_word = tags[0]  # "ohwx"
        other_tags = tags[1:]   # å…¶ä»–æ ‡ç­¾
        
        print(f"åŸºç¡€æç¤º: {base_prompt}")
        print(f"è§¦å‘è¯: {trigger_word}")
        print(f"å¯æ‰“ä¹±æ ‡ç­¾: {other_tags}")
        
        # ç”Ÿæˆå¤šä¸ªæ‰“ä¹±ç‰ˆæœ¬
        shuffled_versions = []
        for i in range(10):  # ç”Ÿæˆ10ä¸ªä¸åŒçš„æ‰“ä¹±ç‰ˆæœ¬
            shuffled_tags = other_tags.copy()
            random.shuffle(shuffled_tags)
            shuffled_prompt = trigger_word + ", " + ", ".join(shuffled_tags)
            shuffled_versions.append(shuffled_prompt)
        
        # æµ‹è¯•æ¦‚å¿µä¸€è‡´æ€§
        print(f"\nğŸ“Š æ‰“ä¹±æ•ˆæœåˆ†æ:")
        
        original_embeddings = {}
        shuffled_embeddings = {}
        
        for model_name in ["TE1", "TE2"]:
            # ç¼–ç åŸå§‹ç‰ˆæœ¬
            original_result = self.encode_text_with_details(base_prompt, model_name)
            original_embeddings[model_name] = original_result['embedding']
            
            # ç¼–ç æ‰“ä¹±ç‰ˆæœ¬
            shuffled_embeddings[model_name] = []
            similarities = []
            
            for i, shuffled_prompt in enumerate(shuffled_versions):
                shuffled_result = self.encode_text_with_details(shuffled_prompt, model_name)
                shuffled_embeddings[model_name].append(shuffled_result['embedding'])
                
                # è®¡ç®—ä¸åŸå§‹ç‰ˆæœ¬çš„ç›¸ä¼¼åº¦
                similarity = cosine_similarity(original_embeddings[model_name], 
                                             shuffled_result['embedding'])[0][0]
                similarities.append(similarity)
                
                if i < 3:  # åªæ˜¾ç¤ºå‰3ä¸ªè¯¦ç»†ä¿¡æ¯
                    print(f"  {model_name} ç‰ˆæœ¬{i+1}: {similarity:.4f}")
                    print(f"    åŸå§‹: {base_prompt[:60]}...")
                    print(f"    æ‰“ä¹±: {shuffled_prompt[:60]}...")
            
            avg_similarity = np.mean(similarities)
            std_similarity = np.std(similarities)
            
            print(f"\n  {model_name} ç»Ÿè®¡:")
            print(f"    å¹³å‡ç›¸ä¼¼åº¦: {avg_similarity:.4f} Â± {std_similarity:.4f}")
            print(f"    ä¸€è‡´æ€§è¯„ä»·: {'ä¼˜ç§€' if avg_similarity > 0.95 else 'è‰¯å¥½' if avg_similarity > 0.90 else 'éœ€è¦æ”¹è¿›'}")
        
        # åˆ†æè§¦å‘è¯ä½ç½®çš„é‡è¦æ€§
        print(f"\nğŸ¯ è§¦å‘è¯ä½ç½®é‡è¦æ€§åˆ†æ:")
        
        # æµ‹è¯•è§¦å‘è¯åœ¨ä¸åŒä½ç½®çš„æ•ˆæœ
        position_tests = [
            f"{trigger_word}, " + ", ".join(other_tags),  # å¼€å¤´
            ", ".join(other_tags[:3]) + f", {trigger_word}, " + ", ".join(other_tags[3:]),  # ä¸­é—´
            ", ".join(other_tags) + f", {trigger_word}"   # ç»“å°¾
        ]
        
        position_names = ["å¼€å¤´", "ä¸­é—´", "ç»“å°¾"]
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} ä½ç½®æµ‹è¯•:")
            baseline_emb = original_embeddings[model_name]
            
            for pos_name, test_prompt in zip(position_names, position_tests):
                result = self.encode_text_with_details(test_prompt, model_name)
                similarity = cosine_similarity(baseline_emb, result['embedding'])[0][0]
                print(f"    è§¦å‘è¯åœ¨{pos_name}: {similarity:.4f}")
        
        return {
            'original_embeddings': original_embeddings,
            'shuffled_embeddings': shuffled_embeddings,
            'shuffled_versions': shuffled_versions
        }
    
    def mystery_3_structured_format_optimization(self):
        """ç–‘æƒ‘3: ç»“æ„åŒ–æ‹¼æ¥çš„æœ€ä¼˜æ ¼å¼æ¢ç´¢"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘3: ç»“æ„åŒ–æ‹¼æ¥çš„æœ€ä¼˜æ ¼å¼æ¢ç´¢") 
        print("="*60)
        
        # åŠ è½½çœŸå®æ•°æ®æ ·æœ¬
        data_path = Path("/root/data/cluster_4")
        txt_files = list(data_path.glob("*.txt"))
        
        if not txt_files:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶")
            return None
        
        # é€‰æ‹©ä¸€ä¸ªæ ·æœ¬
        sample_file = random.choice(txt_files)
        with open(sample_file, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            lines = content.split('\n')
            
            if len(lines) < 2:
                print("âŒ æ ·æœ¬æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
                return None
        
        original_tags = lines[0].strip()
        original_desc = lines[1].strip()
        
        print(f"æµ‹è¯•æ ·æœ¬æ¥æº: {sample_file.name}")
        print(f"åŸå§‹æ ‡ç­¾: {original_tags[:80]}...")
        print(f"åŸå§‹æè¿°: {original_desc[:80]}...")
        
        # å®šä¹‰ä¸åŒçš„ç»“æ„åŒ–æ ¼å¼
        format_templates = {
            "æ–¹æ¡ˆ1-ç®€å•æ‹¼æ¥": f"{original_tags}, {original_desc}",
            "æ–¹æ¡ˆ2-å†’å·åˆ†éš”": f"tags: {original_tags}. description: {original_desc}",
            "æ–¹æ¡ˆ3-ç«–çº¿åˆ†éš”": f"keywords: {original_tags} | scene: {original_desc}",
            "æ–¹æ¡ˆ4-æ‹¬å·ç»“æ„": f"[tags: {original_tags}] [description: {original_desc}]",
            "æ–¹æ¡ˆ5-æ˜ç¡®æ ‡è¯†": f"TAGS: {original_tags}. DESCRIPTION: {original_desc}",
            "æ–¹æ¡ˆ6-è‡ªç„¶è¯­è¨€": f"This image contains {original_tags}. The scene shows {original_desc}",
            "æ–¹æ¡ˆ7-JSONé£æ ¼": f"{{tags: {original_tags}, description: {original_desc}}}",
            "æ–¹æ¡ˆ8-æ¢è¡Œåˆ†éš”": f"{original_tags}\n{original_desc}",
        }
        
        # åŸºå‡†å¯¹æ¯”
        baselines = {
            "ä»…æ ‡ç­¾": original_tags,
            "ä»…æè¿°": original_desc
        }
        
        print(f"\nğŸ“Š ç»“æ„åŒ–æ ¼å¼æ•ˆæœåˆ†æ:")
        
        results = {}
        
        # æµ‹è¯•æ‰€æœ‰æ ¼å¼
        all_formats = {**baselines, **format_templates}
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} æµ‹è¯•ç»“æœ:")
            results[model_name] = {}
            
            # ç¼–ç æ‰€æœ‰æ ¼å¼
            embeddings = {}
            token_counts = {}
            
            for format_name, text in all_formats.items():
                result = self.encode_text_with_details(text, model_name, max_length=77)
                embeddings[format_name] = result['embedding']
                token_counts[format_name] = result['valid_tokens']
                
                print(f"    {format_name}: {result['valid_tokens']} tokens, "
                      f"æˆªæ–­: {'æ˜¯' if result['is_truncated'] else 'å¦'}")
            
            # è®¡ç®—æ ¼å¼é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
            print(f"\n    ç›¸ä¼¼åº¦åˆ†æ ({model_name}):")
            
            # é‡ç‚¹æ¯”è¾ƒï¼šå„ç§ç»“æ„åŒ–æ ¼å¼ä¸åŸºå‡†çš„ç›¸ä¼¼åº¦
            baseline_emb_tags = embeddings["ä»…æ ‡ç­¾"]
            baseline_emb_desc = embeddings["ä»…æè¿°"]
            
            best_format = None
            best_score = 0
            
            for format_name in format_templates.keys():
                format_emb = embeddings[format_name]
                
                # ä¸æ ‡ç­¾çš„ç›¸ä¼¼åº¦
                sim_tags = cosine_similarity(baseline_emb_tags, format_emb)[0][0]
                # ä¸æè¿°çš„ç›¸ä¼¼åº¦  
                sim_desc = cosine_similarity(baseline_emb_desc, format_emb)[0][0]
                # å¹³è¡¡åˆ†æ•°
                balance_score = min(sim_tags, sim_desc)  # å–æœ€å°å€¼ç¡®ä¿ä¸¤è€…éƒ½èƒ½å…¼é¡¾
                
                print(f"      {format_name}:")
                print(f"        ä¸æ ‡ç­¾ç›¸ä¼¼åº¦: {sim_tags:.4f}")
                print(f"        ä¸æè¿°ç›¸ä¼¼åº¦: {sim_desc:.4f}")
                print(f"        å¹³è¡¡åˆ†æ•°: {balance_score:.4f}")
                
                if balance_score > best_score:
                    best_score = balance_score
                    best_format = format_name
                
                results[model_name][format_name] = {
                    'embedding': format_emb,
                    'sim_tags': sim_tags,
                    'sim_desc': sim_desc,
                    'balance_score': balance_score,
                    'token_count': token_counts[format_name]
                }
            
            print(f"\n    ğŸ† {model_name} æœ€ä½³æ ¼å¼: {best_format} (å¹³è¡¡åˆ†æ•°: {best_score:.4f})")
        
        # è·¨æ¨¡å‹åˆ†æ
        print(f"\nğŸ¯ è·¨æ¨¡å‹æ ¼å¼ä¸€è‡´æ€§åˆ†æ:")
        for format_name in format_templates.keys():
            te1_score = results["TE1"][format_name]['balance_score']
            te2_score = results["TE2"][format_name]['balance_score']
            consistency = 1 - abs(te1_score - te2_score)  # ä¸€è‡´æ€§åˆ†æ•°
            
            print(f"  {format_name}: TE1={te1_score:.3f}, TE2={te2_score:.3f}, ä¸€è‡´æ€§={consistency:.3f}")
        
        return results
    
    def mystery_4_overfitting_detection(self):
        """ç–‘æƒ‘4: è¿‡æ‹Ÿåˆæ£€æµ‹çš„å®šé‡æŒ‡æ ‡"""
        print("\n" + "="*60)
        print("ğŸ” ç–‘æƒ‘4: åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„è¿‡æ‹Ÿåˆæ£€æµ‹æ–¹æ³•")
        print("="*60)
        
        # åŠ è½½å¤šä¸ªæ•°æ®æ ·æœ¬ï¼Œæ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„åµŒå…¥å˜åŒ–
        data_path = Path("/root/data/cluster_4")
        txt_files = list(data_path.glob("*.txt"))[:10]  # å–å‰10ä¸ªæ ·æœ¬
        
        if len(txt_files) < 5:
            print("âŒ æ•°æ®æ ·æœ¬æ•°é‡ä¸è¶³")
            return None
        
        print(f"åŠ è½½ {len(txt_files)} ä¸ªæ•°æ®æ ·æœ¬è¿›è¡Œè¿‡æ‹Ÿåˆæ£€æµ‹åˆ†æ")
        
        # å‡†å¤‡æµ‹è¯•æ•°æ®
        samples = []
        for txt_file in txt_files:
            with open(txt_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                lines = content.split('\n')
                if len(lines) >= 2:
                    samples.append({
                        'filename': txt_file.name,
                        'tags': lines[0].strip(),
                        'description': lines[1].strip(),
                        'combined': f"{lines[0].strip()}, {lines[1].strip()}"
                    })
        
        print(f"æˆåŠŸåŠ è½½ {len(samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        
        # æ¨¡æ‹Ÿä¸åŒè®­ç»ƒé˜¶æ®µçš„åµŒå…¥ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™äº›åº”è¯¥æ¥è‡ªè®­ç»ƒcheckpointsï¼‰
        training_stages = ["åˆæœŸ", "ä¸­æœŸ", "åæœŸ", "è¿‡æ‹ŸåˆæœŸ"]
        
        overfitting_metrics = {}
        
        for model_name in ["TE1", "TE2"]:
            print(f"\n  {model_name} è¿‡æ‹Ÿåˆæ£€æµ‹åˆ†æ:")
            
            # è®¡ç®—æ ·æœ¬é—´çš„åŸºç¡€ç›¸ä¼¼åº¦çŸ©é˜µ
            embeddings = []
            for sample in samples:
                result = self.encode_text_with_details(sample['combined'], model_name)
                embeddings.append(result['embedding'].flatten())
            
            embeddings = np.array(embeddings)
            
            # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
            similarity_matrix = cosine_similarity(embeddings)
            
            # è¿‡æ‹Ÿåˆæ£€æµ‹æŒ‡æ ‡
            metrics = self.calculate_overfitting_metrics(similarity_matrix, samples)
            overfitting_metrics[model_name] = metrics
            
            print(f"    å¹³å‡æ ·æœ¬é—´ç›¸ä¼¼åº¦: {metrics['avg_similarity']:.4f}")
            print(f"    ç›¸ä¼¼åº¦æ ‡å‡†å·®: {metrics['similarity_std']:.4f}")
            print(f"    æœ€é«˜ç›¸ä¼¼åº¦: {metrics['max_similarity']:.4f}")
            print(f"    å¤šæ ·æ€§æŒ‡æ•°: {metrics['diversity_index']:.4f}")
            print(f"    è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°: {metrics['overfitting_risk']}")
        
        # å»ºè®®è¿‡æ‹Ÿåˆæ£€æµ‹é˜ˆå€¼
        print(f"\nğŸ“‹ è¿‡æ‹Ÿåˆæ£€æµ‹å»ºè®®:")
        print(f"  1. å¹³å‡æ ·æœ¬é—´ç›¸ä¼¼åº¦ > 0.85: é«˜é£é™©")
        print(f"  2. ç›¸ä¼¼åº¦æ ‡å‡†å·® < 0.05: ç¼ºä¹å¤šæ ·æ€§")
        print(f"  3. æœ€é«˜ç›¸ä¼¼åº¦ > 0.95: å¯èƒ½å­˜åœ¨é‡å¤å­¦ä¹ ")
        print(f"  4. å¤šæ ·æ€§æŒ‡æ•° < 0.3: æ¦‚å¿µåå¡Œé£é™©")
        
        # æä¾›è®­ç»ƒå»ºè®®
        for model_name in ["TE1", "TE2"]:
            metrics = overfitting_metrics[model_name]
            print(f"\n  {model_name} è®­ç»ƒå»ºè®®:")
            
            if metrics['avg_similarity'] > 0.85:
                print(f"    âš ï¸  å¹³å‡ç›¸ä¼¼åº¦è¿‡é«˜ï¼Œå»ºè®®é™ä½å­¦ä¹ ç‡æˆ–å¢åŠ æ•°æ®å¤šæ ·æ€§")
            
            if metrics['similarity_std'] < 0.05:
                print(f"    âš ï¸  ç¼ºä¹å¤šæ ·æ€§ï¼Œå»ºè®®æ·»åŠ æ›´å¤šä¸åŒé£æ ¼çš„è®­ç»ƒæ•°æ®")
            
            if metrics['overfitting_risk'] == "é«˜":
                print(f"    ğŸš¨ é«˜è¿‡æ‹Ÿåˆé£é™©ï¼Œå»ºè®®ç«‹å³åœæ­¢è®­ç»ƒæˆ–å›é€€åˆ°ä¹‹å‰çš„checkpoint")
            elif metrics['overfitting_risk'] == "ä¸­":
                print(f"    âš ï¸  ä¸­ç­‰é£é™©ï¼Œå»ºè®®å¯†åˆ‡ç›‘æ§åç»­è®­ç»ƒ")
            else:
                print(f"    âœ… é£é™©è¾ƒä½ï¼Œå¯ä»¥ç»§ç»­è®­ç»ƒ")
        
        return overfitting_metrics
    
    def calculate_overfitting_metrics(self, similarity_matrix, samples):
        """è®¡ç®—è¿‡æ‹Ÿåˆæ£€æµ‹æŒ‡æ ‡"""
        n = len(similarity_matrix)
        
        # æ’é™¤å¯¹è§’çº¿å…ƒç´ ï¼ˆè‡ªå·±ä¸è‡ªå·±çš„ç›¸ä¼¼åº¦ï¼‰
        mask = ~np.eye(n, dtype=bool)
        similarities = similarity_matrix[mask]
        
        avg_similarity = np.mean(similarities)
        similarity_std = np.std(similarities)
        max_similarity = np.max(similarities)
        
        # å¤šæ ·æ€§æŒ‡æ•°ï¼šæ ‡å‡†å·®é™¤ä»¥å¹³å‡å€¼
        diversity_index = similarity_std / avg_similarity if avg_similarity > 0 else 0
        
        # è¿‡æ‹Ÿåˆé£é™©è¯„ä¼°
        risk_score = 0
        if avg_similarity > 0.85:
            risk_score += 2
        elif avg_similarity > 0.75:
            risk_score += 1
            
        if similarity_std < 0.05:
            risk_score += 2
        elif similarity_std < 0.10:
            risk_score += 1
            
        if max_similarity > 0.95:
            risk_score += 1
        
        if risk_score >= 4:
            overfitting_risk = "é«˜"
        elif risk_score >= 2:
            overfitting_risk = "ä¸­"
        else:
            overfitting_risk = "ä½"
        
        return {
            'avg_similarity': avg_similarity,
            'similarity_std': similarity_std,
            'max_similarity': max_similarity,
            'diversity_index': diversity_index,
            'overfitting_risk': overfitting_risk,
            'risk_score': risk_score
        }

def main():
    print("ğŸš€ å¯åŠ¨SDXLè®­ç»ƒç–‘æƒ‘ç»¼åˆè§£è°œåˆ†æ")
    print("="*60)
    
    analyzer = MysteryAnalyzer()
    
    # æ‰§è¡Œæ‰€æœ‰åˆ†æ
    all_results = {}
    
    try:
        # ç–‘æƒ‘1: æ–‡æœ¬é•¿åº¦å½±å“
        all_results['length_impact'] = analyzer.mystery_1_text_length_impact()
        
        # ç–‘æƒ‘2: shuffle_captionæœºåˆ¶
        all_results['shuffle_mechanism'] = analyzer.mystery_2_shuffle_caption_mechanism()
        
        # ç–‘æƒ‘3: ç»“æ„åŒ–æ ¼å¼ä¼˜åŒ–
        all_results['structured_format'] = analyzer.mystery_3_structured_format_optimization()
        
        # ç–‘æƒ‘4: è¿‡æ‹Ÿåˆæ£€æµ‹
        all_results['overfitting_detection'] = analyzer.mystery_4_overfitting_detection()
        
        # ä¿å­˜ç»“æœ
        with open('/root/mystery_analysis_results.json', 'w', encoding='utf-8') as f:
            # åªä¿å­˜å¯åºåˆ—åŒ–çš„æ•°æ®
            serializable_results = {}
            for key, value in all_results.items():
                if key == 'length_impact':
                    serializable_results[key] = [
                        {k: v for k, v in item.items() if k != 'embedding'}
                        for item in value
                    ]
                elif key in ['structured_format', 'overfitting_detection']:
                    if value is not None:
                        serializable_results[key] = {
                            k: {k2: {k3: v3 for k3, v3 in v2.items() if k3 != 'embedding'} 
                                if isinstance(v2, dict) else v2 
                                for k2, v2 in v.items()} 
                            if isinstance(v, dict) else v 
                            for k, v in value.items()
                        }
                else:
                    serializable_results[key] = value
            
            json.dump(serializable_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ åˆ†æå®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° /root/mystery_analysis_results.json")
        
        # è¾“å‡ºæ€»ç»“
        print(f"\n" + "="*60)
        print("ğŸ“ æ ¸å¿ƒå‘ç°æ€»ç»“:")
        print("="*60)
        print("1. æ–‡æœ¬é•¿åº¦æˆªæ–­ç¡®å®ä¼šå½±å“è¯­ä¹‰ç†è§£ï¼Œä½†å½±å“ç¨‹åº¦å› æ¨¡å‹è€Œå¼‚")
        print("2. shuffle_captionæœºåˆ¶åœ¨ç»´æŒæ¦‚å¿µä¸€è‡´æ€§æ–¹é¢è¡¨ç°è‰¯å¥½")  
        print("3. ç»“æ„åŒ–æ‹¼æ¥æ ¼å¼çš„é€‰æ‹©å¯¹åŒç¼–ç å™¨æ€§èƒ½æœ‰æ˜¾è‘—å½±å“")
        print("4. åŸºäºåµŒå…¥ç›¸ä¼¼åº¦çš„è¿‡æ‹Ÿåˆæ£€æµ‹æ˜¯å¯è¡Œçš„ç›‘æ§æ–¹æ³•")
        print("="*60)
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 